{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Workshop BI com Azure e seus componentes","text":""},{"location":"#apresentacao-do-workshop","title":"Apresenta\u00e7\u00e3o do Workshop","text":"<p>Este Workshop tem como objetivo explorar todas as etapas do fluxo de dados de uma empresa, desde a sua origem at\u00e9 o destino final, utilizando uma esteira de dados no Azure de maneira abrangente e integrada.</p> <p></p> <p>Vou explicar o contexto do projeto que ser\u00e1 desenvolvido durante o Workshop. </p> <p>Primeiro ponto:</p> <ul> <li>01 - Os dados da minha empresa est\u00e3o dispon\u00edveis em um banco de dados OnPremise;</li> </ul> <p>Segundo ponto:</p> <ul> <li> <p>02 - Esses dados dever\u00e3o ser extra\u00eddos do sistema de forma di\u00e1ria no e armazenados em algum lugar;</p> </li> <li> <p>03 - Os dados s\u00e3o divididos em duas categorias: Dados mestres (dados de cliente, loja, vendedor, etc) e dados transacionais (dados de venda);</p> </li> <li> <p>04 - Os dados mestres, devem ser extra\u00eddos todos os dias em sua totalidade, ou seja, full;</p> </li> <li> <p>05 - J\u00e1 os dados transacionais, devem ser extra\u00eddos todos os dias em formato delta;</p> </li> </ul> <p>Terceiro ponto:</p> <ul> <li>06 - Os dados devem ser persistidos na camada Landing Zone em formato parquet;</li> </ul> <p>Quarto ponto</p> <ul> <li> <p>07 - Ap\u00f3s a extra\u00e7\u00e3o, os dados ser\u00e3o processados levando em considera\u00e7\u00e3o a arquitetura medalh\u00e3o. </p> </li> <li> <p>08 - Na camada bronze os dados n\u00e3o dever\u00e3o sofrer nenhum tratamento de dados;</p> </li> <li> <p>09 - Na camada silver, os dados dever\u00e3o passar por transforma\u00e7\u00f5es/tratamentos;</p> </li> <li> <p>10 - Na camada gold, os dados devem estar modelados no padr\u00e3o multidimensional, ou seja, em formato de tabelas de dimens\u00f5es e fato;</p> </li> <li> <p>11 - Ap\u00f3s o processamento, os arquivos precisam ser movidos/arquivados para algum lugar;</p> </li> </ul> <p>Quinto ponto</p> <ul> <li>12 - O desenvolvimento deve contar com uma esteira DevOps. Pensando nisso, iremos avaliar o uso do GitHub ou o Azure DevOps;</li> </ul> <p>Sexto ponto</p> <ul> <li>13 - As credenciais de acesso aos recursos n\u00e3o podem estar dispon\u00edveis. Para atender esse ponto, estaremos usando o Azure Key Vault;</li> </ul> <p>S\u00e9timo ponto</p> <ul> <li>14 - A qualidade dos dados deve ser mantida. E para atender esse pondo, vamos usar t\u00e9cnicas de Data Quality e criar monitoramentos para nossas pipelines.</li> </ul> <p>Como vamos fazer isso tudo acontecer?</p> <p>Vem com a gente!</p>"},{"location":"0_quem_sou_eu/","title":"Workshop BI com Azure e seus componentes","text":""},{"location":"0_quem_sou_eu/#quem-sou-eu","title":"Quem sou eu?","text":"<p>Meu nome \u00e9 Gabriel Quintella e voc\u00ea pode me chamar de Quintell\u00e3o caso queira!</p> <p>Trabalho com dados desde 2005.</p> <p>Sou formado em Sistemas de Informa\u00e7\u00e3o com Especializa\u00e7\u00e3o em Projetos e Bancos de Dados com foco em BI e Bigdata.</p> <p>Durante minha caminhada profissional conquistei as certifica\u00e7\u00f5es Itil, Cobit, Microsoft tanto em ambientes OnPrimise e Cloud (Azure) al\u00e9m de Python em Databricks.</p> <p>Atualmente trabalho como Engenheiro de Dados e durante minha carreira acumulei as fun\u00e7\u00f5es de DBA, Administrador de Dados, Programador SQL como foco em Pl/SQL e TSQL e desenvolvedor de BI focado em SQL Server Integration Services.</p> <p>Possuo experi\u00eancia em lideran\u00e7a de times de dados e proefici\u00eancia em bancos de dados tanto na parte de administra\u00e7\u00e3o quanto desenvolvimento e profundos conhecimentos em modelagem de dados relacional e multidimensional.</p> <p>Quando falamos em ferramentas de integra\u00e7\u00e3o, trabalhei por muitos anos com Pentaho, SQL Server Integration Service e atualmento meu foco \u00e9 com Azure Data Factory.</p> <p>E tamb\u00e9m possuo conhecimento em Python e PySpark.</p> <p>Deixo minhas redes sociais para poder acompanhar meus trabalhos!</p> <p>E-mail: dbaassists@gmail.com  Intagram - https://www.instagram.com/dbaassists/  Twitter - https://twitter.com/dbaassists  YouTube - https://www.youtube.com/@quintellao/featured  Blog DbaAssists - https://www.dbaassists.com.br/  GitHub - https://github.com/dbaassists </p> <p>Grande abra\u00e7o e fique com Deus!!</p> <p>Quintell\u00e3o</p>"},{"location":"10_1_data_quality/","title":"Workshop BI com Azure e seus componentes","text":""},{"location":"10_1_data_quality/#101-data-quality-como-que-funciona","title":"10.1 - Data Quality - Como que funciona?","text":"<p>O processo de Data Quality desenvolvido possui ALGUMAS etapas.</p> <p>1 - Cadastro da Fonte de Dado;</p> <ul> <li>1.1 - Preenchimento de uma planilha com informa\u00e7\u00f5es da fonte;</li> </ul> <p>2 - Valida\u00e7\u00e3o da Fonte</p> <ul> <li>2.1 - Valida\u00e7\u00e3o da planilha preenchida conforme as regras pr\u00e9-definidas.</li> </ul> <p>3 - Cadastro da Fonte no Data Quality</p> <ul> <li>3.1 - Ap\u00f3s valida\u00e7\u00e3o da planilha, a fonte ser\u00e1 cadastrada em nossa solu\u00e7\u00e3o de Data Quality.</li> </ul>"},{"location":"10_1_data_quality/#1-preenchimento-da-planilha","title":"1 - Preenchimento da Planilha","text":"<p>Regras comuns para todas as abas.</p> Seq Etapa Regra Descri\u00e7\u00e3o Seq 01 Valida\u00e7\u00e3o Abas Planilha A planilha deve conter apenas as abas Projeto, FonteDado, EstruturaFonteDado. Caso exista alguma aba que com nomenclatura diferente, o arquivo deve ser rejeitado. Seq 02 Etapas permitidas no fluxo. Cada aba possui uma coluna ETAPA. Nessa coluna s\u00e3o permitidos apenas os seguintes valores CADASTRAR, ALTERAR e EXCLUIR. Caso exista algum valor diferente, o arquivo deve ser rejeitado. Seq 03 Valida\u00e7\u00e3o Planilha sem Dados Todas as abas da planilha s\u00e3o validadas para garantir que estejam preenchidas. Caso alguma aba esteja sem informa\u00e7\u00e3o, o arquivo deve ser rejeitado. Seq 04 Abas Projeto e FonteDados Nessas abas, s\u00f3 \u00e9 permitido a exist\u00eancia de um \u00fanico registro. Caso existam 2 ou mais registros nas abas Projeto e FonteDados, o arquivo deve ser rejeitado."},{"location":"10_1_data_quality/#2-validacao-dos-dados-antes-do-cadastro","title":"2 - Valida\u00e7\u00e3o dos Dados antes do Cadastro","text":""},{"location":"10_1_data_quality/#21-aba-projeto","title":"2.1 - Aba Projeto","text":"Seq Etapa Regra Descri\u00e7\u00e3o Seq 01 Preenchimento Coluna \"Etapa\" Caso esteja assinalado como CADASTRAR, a coluna \"C\u00f3digo Projeto\" n\u00e3o pode estar preenchida. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 02 Preenchimento Coluna \"Etapa\" Caso esteja assinalado como ATUALIZAR ou EXCLUIR, a coluna \"C\u00f3digo Projeto\" dever\u00e1 estar preenchida. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 03 Preenchimento da Coluna \"C\u00f3digo do Projeto\" Essa coluna dever\u00e1 ser preenchida apenas caso a coluna \"Etapa\" esteja preenchida com as informa\u00e7\u00f5es \"ALTERAR\" ou \"EXCLUIR\". O valor dessa coluna \u00e9 gerado no momento do cadastro do projeto. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 04 Preenchimento Coluna \"Categoria Projeto\" Essa coluna dever\u00e1 estar preenchida. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 05 Preenchimento Coluna \"Nome Projeto\" Essa coluna dever\u00e1 estar preenchida. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 06 Preenchimento Coluna \"N\u00famero de Execu\u00e7\u00f5es\" Essa coluna n\u00e3o possui seu preenchimento obrigat\u00f3rio, por\u00e9m caso seja, s\u00e3o aceitos apenas valores inteiros no intervalo de 1 \u00e0 10. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 07 Valida\u00e7\u00e3o do Preenchimento Colunas \"Nome Repons\u00e1vel Projeto\" e \"Nome Respons\u00e1vel T\u00e9cnico\" Essa coluna dever\u00e1 estar preenchida. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 08 Valida\u00e7\u00e3o do Preenchimento Colunas \"Nome Repons\u00e1vel Projeto\" e \"Nome Respons\u00e1vel T\u00e9cnico\" \u00c9 realizada uma valida\u00e7\u00e3o para garantir que um e-mail foi informado. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado."},{"location":"10_1_data_quality/#22-aba-fontedado","title":"2.2 - Aba FonteDado","text":"Seq Etapa Regra Descri\u00e7\u00e3o Seq 01 Preenchimento Coluna \"Etapa\" Caso esteja assinalado como CADASTRAR, a coluna \"C\u00f3digo Projeto\" n\u00e3o pode estar preenchida. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 02 Preenchimento Coluna \"Etapa\" Caso esteja assinalado como ATUALIZAR ou EXCLUIR, a coluna \"C\u00f3digo Projeto\" dever\u00e1 estar preenchida. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 03 Preenchimento da Coluna \"C\u00f3digo da Fonte\" Essa coluna dever\u00e1 ser preenchida apenas caso a coluna \"Etapa\" esteja preenchida com as informa\u00e7\u00f5es \"ALTERAR\" ou \"EXCLUIR\". O valor dessa coluna \u00e9 gerado no momento do cadastro da fonte de dado. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 04 Preenchimento da Coluna \"Nome Fonte Dado\" Essa coluna dever\u00e1 ser preenchida. Essa coluna representa o nome do arquivo que ser\u00e1 importado. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 05 Preenchimento da Coluna \"Breve Descri\u00e7\u00e3o\" Essa coluna dever\u00e1 ser preenchida. Ela funciona como uma forma de dicion\u00e1rio de dados da fonte. Essa coluna possui uma breve descri\u00e7\u00e3o. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 06 Preenchimento da Coluna \"Diret\u00f3rio Raiz Arquivo\" Essa coluna o local onde o arquivo ser\u00e1 disponibilizado no data lake. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 07 Preenchimento da Coluna \"Extens\u00e3o Arquivo\" Essa coluna identifica qual a extens\u00e3o do arquivo que dever\u00e1 ser consumido. Os tipos aceitos s\u00e3o: csv, txt e xlsx Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 08 Preenchimento da Coluna \"Delimitador\" Essa coluna identificar\u00e1 qual o delimitador de coluna para um arquivo. Caso a coluna \"Extens\u00e3o Arquivo\" esteja preenchida com \"csv\" ou \"txt\", essa informa\u00e7\u00e3o \u00e9 obrigat\u00f3ria. Seq 09 Preenchimento da Coluna \"Delimitador\" Essa coluna identificar\u00e1 qual o delimitador de coluna para um arquivo. Caso a coluna \"Extens\u00e3o Arquivo\" esteja preenchida com \"xlsx\" ou \"txt\", essa coluna n\u00e3o dever\u00e1 ser preenchida. Seq 10 Preenchimento da Coluna \"Nome Tabela Data Lake\" Essa coluna dever\u00e1 ser preenchida. Essa coluna identifica o nome da tabela ao qual o dado ser\u00e1 persistida no data lake. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 11 Preenchimento da Coluna \"Aba\" Essa coluna identificar\u00e1 qual \u00e1 e a aba do arquivo do excel (planilha) os dados dever\u00e3o ser consumidos. Caso a coluna \"Extens\u00e3o Arquivo\" esteja preenchida com \"xlsx\", essa coluna tem seu preenchimento obrigat\u00f3rio. Seq 12 Preenchimento da Coluna \"Aba\" Essa coluna identificar\u00e1 qual \u00e1 e a aba do arquivo do excel (planilha) os dados dever\u00e3o ser consumidos. Caso a coluna \"Extens\u00e3o Arquivo\" esteja preenchida com \"csv\" ou \"txt\", n\u00e3o poder\u00e1 ser preenchida. Seq 13 Preenchimento da Coluna \"Encoding\" Essa coluna identifica a codifica\u00e7\u00e3o dos dados. Caso a coluna \"Extens\u00e3o Arquivo\" esteja preenchida com \"csv\" ou \"txt\", essa informa\u00e7\u00e3o \u00e9 obrigat\u00f3ria."},{"location":"10_1_data_quality/#23-aba-estruturafontedado","title":"2.3 - Aba EstruturaFonteDado","text":"Seq Etapa Regra Descri\u00e7\u00e3o Seq 01 Preenchimento Coluna \"Etapa\" Caso esteja assinalado como CADASTRAR, a coluna \"C\u00f3digo Projeto\" n\u00e3o pode estar preenchida. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 02 Preenchimento Coluna \"Etapa\" Caso esteja assinalado como ATUALIZAR ou EXCLUIR, a coluna \"C\u00f3digo Projeto\" dever\u00e1 estar preenchida. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 03 Preenchimento Coluna \"Codigo Projeto\" Essa coluna dever\u00e1 ser preenchida caso a coluna \"Etapa\" esteja preenchida com as informa\u00e7\u00f5es \"ALTERAR\" ou \"EXCLUIR\". O valor dessa coluna \u00e9 gerado no momento do cadastro do projeto. Para identificar qual o c\u00f3digo correto do projeto, basta consultar na base de dados. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 04 Preenchimento Coluna \"Codigo Fonte Dado\" Essa coluna dever\u00e1 ser preenchida apenas caso a coluna \"Etapa\" esteja preenchida com as informa\u00e7\u00f5es \"ALTERAR\" ou \"EXCLUIR\". O valor dessa coluna \u00e9 gerado no momento do cadastro da fonte de dado. Para identificar qual o c\u00f3digo correto da fonte do dado, basta consultar na base de dados. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 05 Preenchimento das Colunas \"Codigo Projeto\" e \"Codigo Fonte Dado\" Essas colunas dever\u00e3o ser preenchida apenas caso a coluna \"Etapa\" esteja preenchida com as informa\u00e7\u00f5es \"ALTERAR\" ou \"EXCLUIR\". O valor dessa coluna \u00e9 gerado no momento do cadastro da fonte de dado. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 06 Preenchimento da Coluna \"Nome Fonte de Dado\" Essa coluna dever\u00e1 ser preenchida com o nome da fonte. Mesma informa\u00e7\u00e3o usada na coluna \"Nome Fonte Dado\" da aba FonteDado Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 07 Preenchimento da coluna \"Sequencia Coluna\" Essa coluna identifica a ordem das colunas dentro da tabela. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 08 Preenchimento da coluna \"Sequencia Coluna\" Essa coluna deve ser preenchida com valores num\u00e9ricos, sequenciais e \u00fanicos Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 09 Preenchimento da coluna \"Nome da Coluna\" Esse coluna deve ser preenchida com o nome da colunas da tabela. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 10 Preenchimento da coluna \"Tipo de Dado\" Essa coluna deve conter apenas os valores INT, STRING, DATETIME , NUMERIC. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 11 Preenchimento da coluna \"Flag Coluna Nula\" Essa coluna dever\u00e1 ser preenchida apenas com valores 0 ou 1. Onde 0 aceita nulo e 1 n\u00e3o aceita nulo. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 12 Preenchimento da coluna \"Flag Coluna Obrigatorio\" Essa coluna dever\u00e1 ser preenchida apenas com valores 0 ou 1. Onde 0 o preenchimento da coluna no arquivo \u00e9 obrigat\u00f3ria e 1 n\u00e3o possui preenchimento obrigat\u00f3rio. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 13 Preenchimento da coluna \"Flag Coluna Chave\" Essa coluna dever\u00e1 ser preenchida apenas com valores 0 ou 1. Onde 0 informa que a coluna faz parte da chave prim\u00e1ria e 1 n\u00e3o n\u00e3o faz parte. Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado. Seq 14 Preenchimento da coluna \"Flag Tipo Dado Armazenado\" Essa coluna \u00e9 usada para identificar o tipo de dado que \u00e9 armazenado em cada coluna. Aceita apenas os valores INTEIRO, NUMERIC, STRING, DATA, EMAIL Caso esteja em desacordo com a regra, o arquivo dever\u00e1 ser rejeitado."},{"location":"10_1_data_quality/#3-cadastro-da-fonte-de-dado","title":"3 - Cadastro da Fonte de Dado","text":""},{"location":"10_data_quality/","title":"Workshop BI com Azure e seus componentes","text":""},{"location":"10_data_quality/#10-data-quality","title":"10 - Data Quality","text":"<p>A garantia da qualidade de dados \u00e9 fundamental para a tomada de decis\u00f5es, an\u00e1lises precisas e confi\u00e1veis, conformidade regulat\u00f3ria e a efic\u00e1cia das opera\u00e7\u00f5es comerciais. </p> <p>As pr\u00e1ticas de Data Quality envolvem a identifica\u00e7\u00e3o, avalia\u00e7\u00e3o e corre\u00e7\u00e3o de problemas de qualidade de dados para garantir que os dados sejam confi\u00e1veis, precisos e \u00fateis para os usu\u00e1rios finais.</p> <p></p> <p>Diante das explica\u00e7\u00f5es acima, Data Quality, ou Qualidade de Dados, refere-se \u00e0 medida em que um conjunto de dados atende aos requisitos e expectativas estabelecidos para sua utiliza\u00e7\u00e3o. Em outras palavras, \u00e9 a capacidade de um conjunto de dados ser preciso, consistente, completo, relevante e atualizado para atender \u00e0s necessidades de neg\u00f3cios e an\u00e1lises.</p> <p>Em nosso Workshop estaremos abordando como que funciona na pr\u00e1tica a sua aplica\u00e7\u00e3o.</p> <p>Voc\u00ea ter\u00e1 acesso ao c\u00f3digo que iremos usar e aplicar em sua empresa.</p> <p></p> Recurso Recurso Nome do Recurso Descri\u00e7\u00e3o Framework Data Quality Framework Uma solu\u00e7\u00e3o de data quality, ou qualidade de dados, refere-se a um conjunto de pr\u00e1ticas, processos e ferramentas utilizadas para garantir que os dados em um sistema ou organiza\u00e7\u00e3o estejam completos, precisos, consistentes, atualizados e relevantes para o prop\u00f3sito pretendido."},{"location":"11_monitoramento_parametrizacao_pipeline/","title":"Workshop BI com Azure e seus componentes","text":""},{"location":"11_monitoramento_parametrizacao_pipeline/#11-monitoramento","title":"11 - Monitoramento","text":""},{"location":"11_monitoramento_parametrizacao_pipeline/#seq-111-monitoramento","title":"SEQ-11.1 - Monitoramento","text":"<p>Nessa etapa irei apresentar para voc\u00ea como que conseguimos monitorar a execu\u00e7\u00e3o de uma pipeline no Azure Data Factoty al\u00e9m de criarmos nosso pr\u00f3prio monitoramento. </p> <p>Podendo gerar relat\u00f3rios de melhorias, focar aten\u00e7\u00e3o para os processos que est\u00e3o apresentando maior incid\u00cancia de falhas e at\u00e9 naqueles processos com alto tempo de processamento.</p>"},{"location":"11_monitoramento_parametrizacao_pipeline/#seq-112-parametrizacao","title":"SEQ-11.2 - Parametriza\u00e7\u00e3o","text":"<p>Quando trabalhamos com pipelines parametrizadas, passamos a ter um processo vers\u00e1til que al\u00e9m de outras caracteristicas importantes, podemos destacar a reutiliza\u00e7\u00e3o de c\u00f3digo, uma vez que o processo foi desenvolvido e \u00e9 orientado a par\u00e2metros.</p> <p></p>"},{"location":"12_gerar_massa_dados/","title":"Workshop BI com Azure e seus componentes","text":""},{"location":"12_gerar_massa_dados/#12-notebook-externo","title":"12 - Notebook Externo","text":"<p>Esse ponto \u00e9 um plus! </p> <p>Usei esse algoritmo para gerar os dados que vamos trabalhar no workshop! </p>"},{"location":"12_gerar_massa_dados/#seq-121-definindo-as-bibliotecas","title":"SEQ-12.1 - Definindo as Bibliotecas","text":"<p>Selecionado quais bibliotecas ser\u00e3o usadas.</p> Bibliotecas Usadas<pre><code>import numpy as np\nfrom faker import Faker\nimport pandas as pd\nfrom pandas import DataFrame\nfrom datetime import datetime\nfrom datetime import date\nimport random\n</code></pre>"},{"location":"12_gerar_massa_dados/#seq-122-massa-de-dados-fakes-de-pessoas","title":"SEQ-12.2 - Massa de Dados Fakes de Pessoas","text":"Cria\u00e7\u00e3o Massa de Dados Fakes de Pessoas<pre><code>print('01 (DEFINICAO) - CRIA\u00c7\u00c3O DA FUN\u00c7\u00c3O QUE GERA MASSA DE DADOS FAKE')\n\ndef geracao_dados_fake(qtd_rows: int = 100, locale: str = \"pt_BR\") -&gt; DataFrame:\n\n    data_atual = datetime.now()\n    local_faker = Faker(locale)\n    list_result = []\n    for _ in range(qtd_rows):\n        data_nascimento = local_faker.date()\n        dict_result = {\n            \"Nome\": local_faker.name(),\n            \"DataNascimento\": str(data_nascimento),\n            'NumIdade': (data_atual.date() - datetime.strptime(data_nascimento, \"%Y-%m-%d\").date()).days //365,\n            \"NomEndereco\": local_faker.street_name(),\n            \"NumEndereco\": local_faker.building_number(),\n            \"NomCidade\": local_faker.city(),\n            \"NumTelefone\": local_faker.phone_number(),\n            \"NomEmail\": local_faker.ascii_email(),\n            \"NomOcupacao\": local_faker.job()\n        }\n        list_result.append(dict_result)\n\n    return pd.DataFrame(list_result)\n\nprint('02 (INICIO) - PROCESSO DE GERA\u00c7\u00c3O DE DADOS DE PESSOAS FAKE')\n\ndfDadosFake = geracao_dados_fake(1500)\n\narquivo = fr'C:\\Temp\\Python_YT\\Arquivo_Exemplo\\exemplo_pessoas.csv'\n\ndfDadosFake.to_csv(arquivo\n                 , sep=';'\n                 , header=True\n                 , index=False)\n\nprint('02 (FIM) - PROCESSO DE GERA\u00c7\u00c3O DE DADOS DE PESSOAS FAKE')\n</code></pre>"},{"location":"12_gerar_massa_dados/#seq-123-massa-de-dados-de-produtos","title":"SEQ-12.3 - Massa de Dados de Produtos","text":"Cria\u00e7\u00e3o Dataframe de Produtos<pre><code>print('03 (INICIO) - PROCESSO DE GERA\u00c7\u00c3O DE DADOS DE PRODUTOS E CATEGORIAS')\n\ndadosProdutos = [('AB\u00d3BORA', 'Verduras e Legumes', 'Verduras e Legumes')\n,('ACELGA', 'Verduras e Legumes', 'Verduras e Legumes')\n,('AGRI\u00c3O', 'Verduras e Legumes', 'Verduras e Legumes')\n,('ALCACHOFRA', 'Verduras e Legumes', 'Verduras e Legumes')\n,('ALFACE', 'Verduras e Legumes', 'Verduras e Legumes')\n,('ALHO-POR\u00d3', 'Verduras e Legumes', 'Verduras e Legumes')\n,('ALMEIR\u00c3O', 'Verduras e Legumes', 'Verduras e Legumes')\n,('ASPARGO', 'Verduras e Legumes', 'Verduras e Legumes')\n,('BATATA', 'Verduras e Legumes', 'Verduras e Legumes')\n,('BERINJELA', 'Verduras e Legumes', 'Verduras e Legumes')\n,('BERTALHA', 'Verduras e Legumes', 'Verduras e Legumes')\n,('BETERRABA', 'Verduras e Legumes', 'Verduras e Legumes')\n,('BR\u00d3COLIS', 'Verduras e Legumes', 'Verduras e Legumes')\n,('CEBOLA', 'Verduras e Legumes', 'Verduras e Legumes')\n,('CENOURA', 'Verduras e Legumes', 'Verduras e Legumes')\n,('CHIC\u00d3RIA', 'Verduras e Legumes', 'Verduras e Legumes')\n,('CHUCHU', 'Verduras e Legumes', 'Verduras e Legumes')\n,('COUVE', 'Verduras e Legumes', 'Verduras e Legumes')\n,('COUVE\u2013FLOR', 'Verduras e Legumes', 'Verduras e Legumes')\n,('JIL\u00d3', 'Verduras e Legumes', 'Verduras e Legumes')\n,('LENTILHA', 'Verduras e Legumes', 'Verduras e Legumes')\n,('MANDIOCA', 'Verduras e Legumes', 'Verduras e Legumes')\n,('MAXIXE', 'Verduras e Legumes', 'Verduras e Legumes')\n,('MILHO VERDE', 'Verduras e Legumes', 'Verduras e Legumes')\n,('MOSTARDA', 'Verduras e Legumes', 'Verduras e Legumes')\n,('NABO', 'Verduras e Legumes', 'Verduras e Legumes')\n,('PALMITO', 'Verduras e Legumes', 'Verduras e Legumes')\n,('PEPINO', 'Verduras e Legumes', 'Verduras e Legumes')\n,('PIMENT\u00c3O', 'Verduras e Legumes', 'Verduras e Legumes')\n,('QUIABO', 'Verduras e Legumes', 'Verduras e Legumes')\n,('RABANETE', 'Verduras e Legumes', 'Verduras e Legumes')\n,('TOMATE', 'Verduras e Legumes', 'Verduras e Legumes')\n,('VAGEM', 'Verduras e Legumes', 'Verduras e Legumes')\n,('Fil\u00e9 Mignon', 'Carne Boi', 'Carnes')\n,('Maminha', 'Carne Boi', 'Carnes')\n,('Picanha', 'Carne Boi', 'Carnes')\n,('Cox\u00e3o mole', 'Carne Boi', 'Carnes')\n,('Patinho', 'Carne Boi', 'Carnes')\n,('Lagarto', 'Carne Boi', 'Carnes')\n,('Contrafil\u00e9', 'Carne Boi', 'Carnes')\n,('Alcatra', 'Carne Boi', 'Carnes')\n,('Costela', 'Carne Boi', 'Carnes')\n,('Paleta', 'Carne Boi', 'Carnes')\n,('Chuleta', 'Carne Boi', 'Carnes')\n,('Cox\u00e3o Duro', 'Carne Boi', 'Carnes')\n,('Fraldinha', 'Carne Boi', 'Carnes')\n,('Ac\u00e9m', 'Carne Boi', 'Carnes')\n,('M\u00fasculo dianteiro', 'Carne Boi', 'Carnes')\n,('Coxa', 'Carne Frango', 'Carnes')\n,('Coxinha da asa', 'Carne Frango', 'Carnes')\n,('Coxa com sobrecoxa', 'Carne Frango', 'Carnes')\n,('Cora\u00e7\u00e3o', 'Carne Frango', 'Carnes')\n,('Sobrecoxa', 'Carne Frango', 'Carnes')\n,('Sassami', 'Carne Frango', 'Carnes')\n,('Fil\u00e9 de sobrecoxa', 'Carne Frango', 'Carnes')\n,('Fil\u00e9 de peito', 'Carne Frango', 'Carnes')\n,('Salm\u00e3o', 'Carne Peixe', 'Carnes')\n,('Bacalhau', 'Carne Peixe', 'Carnes')\n,('Atum', 'Carne Peixe', 'Carnes')\n,('Sardinha', 'Carne Peixe', 'Carnes')\n,('Merluza', 'Carne Peixe', 'Carnes')\n,('Corvina', 'Carne Peixe', 'Carnes')\n,('Camar\u00e3o', 'Carne Peixe', 'Carnes')\n,('Pescadinha', 'Carne Peixe', 'Carnes')\n,('Johnnie Walker', 'Whisky','Bebidas')\n,('Smirnoff', 'Vodka','Bebidas')\n,('Bacardi', 'Rum','Bebidas')\n,('Jack Daniels', 'Whisky','Bebidas')\n,('Absolut', 'Vodka','Bebidas')\n,('Martini', 'Rum','Bebidas')\n,('Chivas Regal', 'Whisky','Bebidas')\n,('Ballantines', 'Whisky','Bebidas')\n,('Mo\u00ebt &amp; Chandon', 'Champagne','Bebidas')\n,('Gordons', 'Gin','Bebidas')\n,('Grants', 'Whisky','Bebidas')\n,('Skyy', 'Vodka','Bebidas')\n,('C\u00eeroc', 'Vodka','Bebidas')\n,('Malibu', 'Rum','Bebidas')\n,('Tanqueray', 'Gin','Bebidas')\n,('Coca-Cola', 'Refrigerante','Bebidas')\n,('Coca-Cola Zero', 'Refrigerante','Bebidas')\n,('Guaran\u00e1 Antarctica', 'Refrigerante','Bebidas')\n,('Mineirinho', 'Refrigerante','Bebidas')\n,('Pepsi', 'Refrigerante','Bebidas')\n,('Pepsi Zero', 'Refrigerante','Bebidas')\n,('Pepsi Twister', 'Refrigerante','Bebidas')\n,('Fanta Laranja', 'Refrigerante','Bebidas')\n,('Fanta Uva', 'Refrigerante','Bebidas')\n,('Sprite', 'Refrigerante','Bebidas')\n,('Sprite Zero', 'Refrigerante','Bebidas')\n,('Kuat', 'Refrigerante','Bebidas')]\n\ndfProdutos = pd.DataFrame(dadosProdutos, columns=['DESCRICAO_PRODUTO' ,'TIPO_PRODUTO',  'CATEGORIA_PRODUTO'])\n\ndfProdutos = dfProdutos.sort_values(by=['CATEGORIA_PRODUTO','DESCRICAO_PRODUTO'])\n\ndfProdutos = dfProdutos.reset_index()\n\ndfProdutos['CODIGO_PRODUTO'] = np.arange(1, len(dfProdutos) + 1)\n\nprint('03.1 (INICIO) - GERA\u00c7\u00c3O DE DADOS DE CATEGORIA DE PRODUTOS')\n\ndfCategoriaProduto =  pd.DataFrame(columns=['CODIGO_CATEGORIA','CATEGORIA_PRODUTO','SUB_CATEGORIA_PRODUTO'])\n\ndfCategoriaProduto_temp = pd.DataFrame()\n\nlistaCategoriaProduto = dfProdutos[['CATEGORIA_PRODUTO','TIPO_PRODUTO']].drop_duplicates()\n\ncodigo_categoria = 1\n\ntype(listaCategoriaProduto)\n\n#for coluna in listaCategoriaProduto.columns:\n\nfor index, row in listaCategoriaProduto.iterrows():\n\n    print(row['CATEGORIA_PRODUTO'])\n    print(row['TIPO_PRODUTO'])\n\n    dfCategoriaProduto_temp = pd.DataFrame({'CODIGO_CATEGORIA': [int(codigo_categoria)]\n                                                ,'CATEGORIA_PRODUTO': [str(row['CATEGORIA_PRODUTO'])]\n                                                ,'SUB_CATEGORIA_PRODUTO': [str(row['TIPO_PRODUTO'])]}\n                                                )\n\n    dfCategoriaProduto = dfCategoriaProduto.append(dfCategoriaProduto_temp)\n\n    codigo_categoria += 1\n\ndfCategoriaProduto.to_csv(fr'C:\\Temp\\Python_YT\\Git\\Projeto_BI_Zero_TO_DW\\01_SCRIPT_SQL\\DADOS_BASE\\08_categoria_produtos.csv'\n                  ,sep=';'\n                  ,header=True\n                  ,index=False)\n\nprint('03.1 (FIM) - GERA\u00c7\u00c3O DE DADOS DE CATEGORIA DE PRODUTOS')\n\nprint('03.2 (INICIO) - GERA\u00c7\u00c3O DE DADOS DE PRODUTOS')\n\ndfProdutos = pd.merge(dfProdutos\n                      , dfCategoriaProduto\n                              ,how='inner'\n                              ,left_on=['CATEGORIA_PRODUTO','TIPO_PRODUTO']\n                              ,right_on=['CATEGORIA_PRODUTO','SUB_CATEGORIA_PRODUTO']\n                      )[['CODIGO_PRODUTO'\n                        ,'DESCRICAO_PRODUTO'\n                        ,'CODIGO_CATEGORIA']]\n\ndfProdutos.to_csv(fr'C:\\Temp\\Python_YT\\Git\\Projeto_BI_Zero_TO_DW\\01_SCRIPT_SQL\\DADOS_BASE\\02_produtos.csv'\n                  ,sep=';'\n                  ,header=True\n                  ,index=False)\n\nprint('03.2 (FIM) - GERA\u00c7\u00c3O DE DADOS DE PRODUTOS')\n\nprint('03 (INICIO) - PROCESSO DE GERA\u00c7\u00c3O DE DADOS DE PRODUTOS E CATEGORIAS')\n</code></pre>"},{"location":"12_gerar_massa_dados/#seq-124-massa-de-dados-de-cliente","title":"SEQ-12.4 - Massa de Dados de Cliente","text":"Cria\u00e7\u00e3o Dataframe de Clientes<pre><code>print('04 (INICIO) - PROCESSO GERA\u00c7\u00c3O DE DADOS DE CLIENTES')\n\ndf = pd.read_csv(f'C:\\Temp\\Python_YT\\Arquivo_Exemplo\\exemplo_pessoas.csv' , sep=';')\n\ndfPessoas = df.loc[1:1398,['Nome','DataNascimento','NumIdade','NomEndereco','NumEndereco','NomCidade','NumTelefone','NomEmail','NomOcupacao']]\n\ndfPessoas = dfPessoas.rename(columns={'Nome':'NOME_CLIENTE'\n                                      ,'DataNascimento':'DATA_NASCIMENTO'\n                                      ,'NumIdade':'IDADE_CLIENTE'\n                                      ,'NomEndereco':'DESCRICAO_ENDERECO'\n                                      ,'NumEndereco':'NUMERO_ENDERECO'\n                                      ,'NomCidade':'NOME_CIDADE'\n                                      ,'NumTelefone':'NUMERO_TELEFONE'\n                                      ,'NomEmail':'EMAIL_CLIENTE'\n                                      ,'NomOcupacao':'OCUPACAO_CLIENTE'})\n\ndfPessoas = dfPessoas.sort_values(by='NOME_CLIENTE')\n\ndfPessoas['CODIGO_CLIENTE'] = np.arange(1, len(dfPessoas) + 1)\n\ndfPessoas = dfPessoas[['CODIGO_CLIENTE'\n                        ,'NOME_CLIENTE'\n                        ,'DATA_NASCIMENTO'\n                        ,'IDADE_CLIENTE'\n                        ,'DESCRICAO_ENDERECO'\n                        ,'NUMERO_ENDERECO'\n                        ,'NOME_CIDADE'\n                        ,'NUMERO_TELEFONE'\n                        ,'EMAIL_CLIENTE'\n                        ,'OCUPACAO_CLIENTE']]\n\ndfPessoas.to_csv(fr'C:\\Temp\\Python_YT\\Git\\Projeto_BI_Zero_TO_DW\\01_SCRIPT_SQL\\DADOS_BASE\\01_clientes.csv'\n                  ,sep=';'\n                  ,header=True\n                  ,index=False)\n\nprint('04 (FIM) - PROCESSO GERA\u00c7\u00c3O DE DADOS DE CLIENTES')\n</code></pre>"},{"location":"12_gerar_massa_dados/#seq-125-massa-de-dados-de-vendedor","title":"SEQ-12.5 - Massa de Dados de Vendedor","text":"Cria\u00e7\u00e3o Dataframe de Vendedores<pre><code>print('05 (INICIO) - PROCESSO GERA\u00c7\u00c3O DE DADOS DE VENDEDORES')\n\ndfPessoasVendedor = df.loc[1400:1438,['Nome']] #.sort_values(by=['Nome'])\ndfVendedor = pd.DataFrame(columns=['CODIGO_VENDEDOR','NOME_VENDEDOR'])\n\ndfVendedor_ecommerce = pd.DataFrame({'CODIGO_VENDEDOR': [0]\n                                            ,'NOME_VENDEDOR':['E-Commerce']})\n\ndfVendedor = dfVendedor.append(dfVendedor_ecommerce, ignore_index=True)\n\ncodigo_vendedor_inicial = 1\ncodigo_vendedor_final = len(dfVendedor)-1\n\nfor vendedor in dfPessoasVendedor['Nome']:\n\n    dados = [{'CODIGO_VENDEDOR' : codigo_vendedor_inicial,'NOME_VENDEDOR': vendedor}]       \n\n    df1 = pd.DataFrame(dados)   \n\n    dfVendedor =  pd.concat([dfVendedor, df1])\n\n    codigo_vendedor_inicial = codigo_vendedor_inicial + 1  \n\ndfVendedor.set_index('CODIGO_VENDEDOR', inplace=True)\n\ndfVendedor.reset_index(inplace=True)\n\ndfVendedor.to_csv(fr'C:\\Temp\\Python_YT\\Git\\Projeto_BI_Zero_TO_DW\\01_SCRIPT_SQL\\DADOS_BASE\\03_vendedores.csv'\n                  ,sep=';'\n                  ,header=True\n                  ,index=False)\n\nprint('05 (FIM) - PROCESSO GERA\u00c7\u00c3O DE DADOS DE VENDEDORES')\n</code></pre>"},{"location":"12_gerar_massa_dados/#seq-126-massa-de-dados-de-forma-de-pagamento","title":"SEQ-12.6 - Massa de Dados de Forma de Pagamento","text":"Cria\u00e7\u00e3o Dataframe de Forma de Pagamento<pre><code>print('06 (INICIO) - PROCESSO GERA\u00c7\u00c3O DE DADOS DE FORMA DE PAGAMENTO')\n\ndadosFormaPagamento = ['CART\u00c3O D\u00c9BITO'\n,'PIX'\n,'CART\u00c3O CR\u00c9DITO'\n,'BOLETO']\n\ndfFormaPagamento = pd.DataFrame(dadosFormaPagamento, columns=['DESCRICAO_FORMA_PAGAMENTO'])\n\ndfFormaPagamento['CODIGO_FORMA_PAGAMENTO'] = np.arange(1, len(dfFormaPagamento) + 1)\n\ndfFormaPagamento.to_csv(fr'C:\\Temp\\Python_YT\\Git\\Projeto_BI_Zero_TO_DW\\01_SCRIPT_SQL\\DADOS_BASE\\05_forma_pagamento.csv'\n                  ,sep=';'\n                  ,header=True\n                  ,index=False)\n\nprint('06 (FIM) - PROCESSO GERA\u00c7\u00c3O DE DADOS DE FORMA DE PAGAMENTO')\n</code></pre>"},{"location":"12_gerar_massa_dados/#seq-127-massa-de-dados-de-loja","title":"SEQ-12.7 - Massa de Dados de Loja","text":"Cria\u00e7\u00e3o Dataframe de Loja<pre><code>print('07 (INICIO) - PROCESSO GERA\u00c7\u00c3O DE DADOS DE LOJAS')\n\ndadosloja = [\n('E-COMMERCE','WWW','VIRTUAL'),\n('LOJA A','MT','F\u00cdSICA'),\n('LOJA B','RJ','F\u00cdSICA'),\n('LOJA C','CE','F\u00cdSICA')\n]\n\nlista_vendedores = []\n\ndfLoja = pd.DataFrame(dadosloja, columns=['NOME_LOJA', 'LOCALIDADE_LOJA', 'TIPO_LOJA'])\n\ndfLoja['CODIGO_LOJA'] = np.arange(1, len(dfLoja) + 1)\n\ndfLojaFinal = pd.DataFrame(columns=['CODIGO_LOJA','NOME_LOJA', 'LOCALIDADE_LOJA', 'TIPO_LOJA', 'CODIGO_VENDEDOR'])\n\nfor index in dfLoja.index:\n\n    print(index)\n\n    if dfLoja['NOME_LOJA'][index] == 'E-COMMERCE':\n\n        dfLojaFinal_vendedor = pd.DataFrame({'CODIGO_LOJA': [dfLoja['CODIGO_LOJA'][index]]\n                                            ,'NOME_LOJA': [dfLoja['NOME_LOJA'][index]]\n                                            ,'LOCALIDADE_LOJA':[dfLoja['LOCALIDADE_LOJA'][index]]\n                                            ,'TIPO_LOJA':[dfLoja['TIPO_LOJA'][index]]\n                                            ,'CODIGO_VENDEDOR':[0]}\n                                            )\n\n        dfLojaFinal =  pd.concat([dfLojaFinal, dfLojaFinal_vendedor])\n\n    else:\n\n        indice_vendedor_randomico = np.random.choice(dfVendedor[dfVendedor['CODIGO_VENDEDOR'] != 0].index, size = random.randint(5, 9) , replace=False)\n\n        for ivr in indice_vendedor_randomico:\n\n            if ivr not in lista_vendedores:\n\n                dfLojaFinal_vendedor = pd.DataFrame({'CODIGO_LOJA': [dfLoja['CODIGO_LOJA'][index]]\n                                                    ,'NOME_LOJA': [dfLoja['NOME_LOJA'][index]]\n                                                    ,'LOCALIDADE_LOJA':[dfLoja['LOCALIDADE_LOJA'][index]]\n                                                    ,'TIPO_LOJA':[dfLoja['TIPO_LOJA'][index]]\n                                                    ,'CODIGO_VENDEDOR':[ivr]}\n                                                    )\n\n                dfLojaFinal =  pd.concat([dfLojaFinal, dfLojaFinal_vendedor])                   \n\n                lista_vendedores.append(ivr)    \n\ndfLojaFinal = dfLojaFinal[['CODIGO_LOJA'\n                          , 'CODIGO_VENDEDOR'\n                          , 'NOME_LOJA'\n                          , 'LOCALIDADE_LOJA'\n                          , 'TIPO_LOJA']].sort_values(by=['CODIGO_LOJA','CODIGO_VENDEDOR'], ascending=[True, True])\n\ndfLojaFinal['CODIGO_LOJA_VENDEDOR'] = np.arange(0, len(dfLojaFinal))\n\ndfLojaFinal[['CODIGO_LOJA_VENDEDOR'\n            , 'CODIGO_LOJA'\n            , 'CODIGO_VENDEDOR'\n            , 'NOME_LOJA'\n            , 'LOCALIDADE_LOJA'\n            , 'TIPO_LOJA']].to_csv(fr'C:\\Temp\\Python_YT\\Git\\Projeto_BI_Zero_TO_DW\\01_SCRIPT_SQL\\DADOS_BASE\\04_lojas.csv'\n                  ,sep=';'\n                  ,header=True\n                  ,index=False)\n\ndfLojaFinal = dfLojaFinal.reset_index()\n\nprint('07 (FIM) - PROCESSO GERA\u00c7\u00c3O DE DADOS DE LOJAS')\n</code></pre>"},{"location":"12_gerar_massa_dados/#seq-128-massa-de-dados-de-venda","title":"SEQ-12.8 - Massa de Dados de Venda","text":"Cria\u00e7\u00e3o Dataframe de Venda<pre><code>print('08 (INICIO) - PROCESSO GERA\u00c7\u00c3O DE DADOS DE VENDAS')\n\ndfFinal = pd.DataFrame(columns=['CODIGO_VENDA'\n                    ,'DATA_VENDA'\n                    ,'CODIGO_CLIENTE'\n                    ,'NOME_CLIENTE'\n                    ,'CODIGO_PRODUTO'\n                    ,'DESCRICAO_PRODUTO'\n                    ,'CODIGO_CATEGORIA'\n                    ,'TIPO_PRODUTO'\n                    ,'CODIGO_LOJA_VENDEDOR'\n                    ,'NOME_VENDEDOR'\n                    ,'CODIGO_LOJA'\n                    ,'NOME_LOJA'\n                    ,'LOCALIDADE_LOJA'\n                    ,'TIPO_LOJA'\n                    ,'VALOR_UNITARIO'\n                    ,'QUANTIDADE'\n                    ,'VALOR_FINAL'\n                    ,'TIPO_PAGAMENTO'])\n\ncodigo_venda_inicial = 1\ncodigo_venda_final = 2542 #43209\n\nwhile codigo_venda_final &gt; 0:\n\n    print('codigo_venda_final: ' + str(codigo_venda_final))\n\n    data_inicio = pd.to_datetime('2017-01-02')\n    data_fim = pd.to_datetime(date.today().isoformat())\n\n    pessoa = int(np.random.choice(dfPessoas.index, size = 1 , replace=False))\n    indice_randomico_vendedor = int(np.random.choice(dfVendedor.index, size = 1 , replace=False))\n    indice_randomico_forma_pagamento = int(np.random.choice(dfFormaPagamento.index, size = 1 , replace=False))    \n\n    indice_randomico_loja = int(np.random.choice(dfLojaFinal.index, size = 1 , replace=False))    \n\n    v_vendedor = dfVendedor['NOME_VENDEDOR'][indice_randomico_vendedor]\n    v_codigo_vendedor = dfVendedor['CODIGO_VENDEDOR'][indice_randomico_vendedor]\n\n    v_codigo_loja = dfLojaFinal['CODIGO_LOJA_VENDEDOR'][indice_randomico_loja]\n\n    v_loja = dfLojaFinal['NOME_LOJA'][indice_randomico_loja]\n    v_lojalocalidade = dfLojaFinal['LOCALIDADE_LOJA'][indice_randomico_loja]   \n    v_lojatipo = dfLojaFinal['TIPO_LOJA'][indice_randomico_loja]       \n    v_formaPagamento = dfFormaPagamento['DESCRICAO_FORMA_PAGAMENTO'][indice_randomico_forma_pagamento]\n    v_codigo_formaPagamento = dfFormaPagamento['CODIGO_FORMA_PAGAMENTO'][indice_randomico_forma_pagamento]\n\n    v_pessoa = dfPessoas['NOME_CLIENTE'][pessoa]\n    v_codigo_cliente = dfPessoas['CODIGO_CLIENTE'][pessoa]\n\n    indice_randomico = np.random.choice(dfProdutos.index, size = random.randint(1, 10) , replace=False)    \n\n    p_data_venda = pd.to_datetime(np.random.uniform(data_inicio.value, data_fim.value, size=365))\n\n    if v_codigo_loja == 0:\n\n        v_data_venda = p_data_venda[0].strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    elif v_codigo_loja != 1:\n\n        hora = int(random.uniform(9, 18))\n        minuto = int(random.uniform(0, 59))\n        segundo = int(random.uniform(0, 59))\n\n        data = str(p_data_venda[0].year) + '-' + str(p_data_venda[0].month).zfill(2) + '-' + str(p_data_venda[0].day).zfill(2) + ' ' + str(hora).zfill(2) + ':' + str(minuto).zfill(2) + ':' + str(segundo).zfill(2)\n\n        v_data_venda = datetime.strptime(data, \"%Y-%m-%d %H:%M:%S\")\n\n    tipo_pagamento = random.randint(1, 4)\n\n    if tipo_pagamento == 3:\n\n        v_tipo_pagamento = 'PARCELADO'\n\n    else:\n\n        v_tipo_pagamento = 'A VISTA'        \n\n    for i in indice_randomico:\n\n        if dfProdutos['CODIGO_CATEGORIA'][i] == 3: #'Whisky':\n\n            valor_min = 89.98\n            valor_max = 179.99\n\n            valor = np.random.uniform(valor_min, valor_max, size=1)\n\n        elif dfProdutos['CODIGO_CATEGORIA'][i] == 1: #'Vodka':\n\n            valor_min = 35.99\n            valor_max = 99.99\n\n            valor = np.random.uniform(valor_min, valor_max, size=1)\n\n        elif dfProdutos['CODIGO_CATEGORIA'][i] == 4: #'Refrigerante':\n\n            valor_min = 6.00\n            valor_max = 9.99\n\n            valor = np.random.uniform(valor_min, valor_max, size=1)\n\n        elif dfProdutos['CODIGO_CATEGORIA'][i] == 2: #'Rum':\n\n            valor_min = 69.00\n            valor_max = 99.99\n\n            valor = np.random.uniform(valor_min, valor_max, size=1)\n\n        elif dfProdutos['CODIGO_CATEGORIA'][i] == 6: #'Champagne':\n\n            valor_min = 115.00\n            valor_max = 159.99\n\n            valor = np.random.uniform(valor_min, valor_max, size=1)            \n\n        elif dfProdutos['CODIGO_CATEGORIA'][i] == 5: #'Gin':\n\n            valor_min = 99.00\n            valor_max = 119.99\n\n            valor = np.random.uniform(valor_min, valor_max, size=1)    \n\n        elif dfProdutos['CODIGO_CATEGORIA'][i] == 10: #'Verduras e Legumes':\n\n            valor_min = 0.99\n            valor_max = 5.99\n\n            valor = np.random.uniform(valor_min, valor_max, size=1)    \n\n        elif dfProdutos['CODIGO_CATEGORIA'][i] == 7: #'Carne Boi':\n\n            valor_min = 29.99\n            valor_max = 75.99\n\n            valor = np.random.uniform(valor_min, valor_max, size=1) \n\n        elif dfProdutos['CODIGO_CATEGORIA'][i] == 9: #'Carne Frango':\n\n            valor_min = 14.99\n            valor_max = 29.99\n\n            valor = np.random.uniform(valor_min, valor_max, size=1) \n\n        elif dfProdutos['CODIGO_CATEGORIA'][i] ==  8: #'Carne Peixe':\n\n            valor_min = 29.99\n            valor_max = 85.99\n\n            valor = np.random.uniform(valor_min, valor_max, size=1)             \n\n        else:\n\n            valor_min = 0.99\n            valor_max = 7.99\n\n            valor = np.random.uniform(valor_min, valor_max, size=1)            \n\n        v_quantidade = int(np.random.randint(1, 10, size=1))\n\n        v_produto = dfProdutos['DESCRICAO_PRODUTO'][i]\n        v_codigo_produto = dfProdutos['CODIGO_PRODUTO'][i]\n        v_tipo_produto = dfProdutos['CODIGO_CATEGORIA'][i]\n\n        dados = [{ 'CODIGO_VENDA' : codigo_venda_inicial\n                    ,'DATA_VENDA': str(v_data_venda)\n                    ,'CODIGO_CLIENTE': v_codigo_cliente\n                    ,'NOME_CLIENTE': v_pessoa\n                    ,'CODIGO_PRODUTO' : v_codigo_produto\n                    ,'DESCRICAO_PRODUTO' : v_produto\n                    ,'CODIGO_VENDEDOR': v_codigo_vendedor\n                    ,'NOME_VENDEDOR' : v_vendedor\n                    ,'CODIGO_LOJA_VENDEDOR': v_codigo_loja\n                    ,'NOME_LOJA': v_loja\n                    ,'LOCALIDADE_LOJA': v_lojalocalidade\n                    ,'TIPO_LOJA': v_lojatipo\n                    ,'VALOR_UNITARIO': round(float(valor),2)\n                    ,'QUANTIDADE': v_quantidade\n                    ,'VALOR_FINAL': round((round(float(valor),2) * v_quantidade),2)\n                    ,'FORMA_PAGAMENTO' : int(v_codigo_formaPagamento)\n                    ,'TIPO_PAGAMENTO' : v_tipo_pagamento}]       \n\n        df = pd.DataFrame(dados)\n\n        dfFinal =  pd.concat([dfFinal, df])\n\n    codigo_venda_inicial = codigo_venda_inicial + 1\n\n    codigo_venda_final = codigo_venda_final - 1\n\nprint('08 (FIM) - PROCESSO GERA\u00c7\u00c3O DE DADOS DE VENDAS')\n\nprint('09 (INICIO) - PROCESSO GERA\u00c7\u00c3O DE DADOS DE VENDAS')\n\ndfVenda = (dfFinal.groupby(['CODIGO_VENDA'\n                            ,'DATA_VENDA'\n                            ,'CODIGO_CLIENTE'\n                            ,'CODIGO_LOJA_VENDEDOR'\n                            ,'FORMA_PAGAMENTO'\n                            ,'TIPO_PAGAMENTO'], as_index=False)\n                .agg(VALOR_FINAL=('VALOR_FINAL','sum'))\n                     .sort_values(by=['DATA_VENDA','CODIGO_VENDA'])\n                     .reset_index(drop=True))\n\ndfVenda.to_csv(fr'C:\\Temp\\Python_YT\\Git\\Projeto_BI_Zero_TO_DW\\01_SCRIPT_SQL\\DADOS_BASE\\06_vendas.csv'\n                  ,sep=';'\n                  ,header=True\n                  ,index=False)\n\nprint('09 (FIM) - PROCESSO GERA\u00c7\u00c3O DE DADOS DE VENDAS')\n\nprint('10 (INICIO) - PROCESSO GERA\u00c7\u00c3O DE DADOS DE ITENS DE VENDAS')\n\ndfItemVenda = (dfFinal.groupby(['CODIGO_VENDA', 'CODIGO_PRODUTO'], as_index=False)\n                .agg(VALOR_UNITARIO=('VALOR_UNITARIO','sum')\n                     ,QUANTIDADE=('QUANTIDADE','sum')\n                     ,VALOR_FINAL=('VALOR_FINAL','sum'))\n                     .sort_values(by=['CODIGO_VENDA', 'CODIGO_PRODUTO'])\n                     .reset_index(drop=True))\n\ndfItemVenda.to_csv(fr'C:\\Temp\\Python_YT\\Git\\Projeto_BI_Zero_TO_DW\\01_SCRIPT_SQL\\DADOS_BASE\\07_item_vendas.csv'\n                  ,sep=';'  \n                  ,header=True\n                  ,index=False)\n\nprint('10 (FIM) - PROCESSO GERA\u00c7\u00c3O DE DADOS DE ITENS DE VENDAS')\n</code></pre>"},{"location":"13_carga_sql_server_relacional/","title":"Workshop BI com Azure e seus componentes","text":""},{"location":"13_carga_sql_server_relacional/#13-notebook-carga-de-dados-no-axure-sql-server","title":"13 - Notebook Carga de Dados no Axure SQL Server","text":"<p>Ap\u00f3s gerar os dados, estarei usando esse algoritmo para carregar os dados no SQL Server!</p>"},{"location":"13_carga_sql_server_relacional/#seq-131-definindo-as-bibliotecas","title":"SEQ-13.1 - Definindo as Bibliotecas","text":"<p>Selecionado quais bibliotecas ser\u00e3o usadas.</p> Bibliotecas Usadas<pre><code>import pandas as pd\nfrom sqlalchemy import create_engine\nimport pyodbc\nimport os\n\nserver = 'localhost\\DBAASSISTS'\ndatabase = 'TREINAMENTO' \nusername = 'usr_treinamento' \npassword = 'Tr&amp;in@M&amp;nT0SQL2024!' \ndriver = 'SQL Server'\n</code></pre>"},{"location":"13_carga_sql_server_relacional/#seq-132-carga-de-clientes","title":"SEQ-13.2 - Carga de Clientes","text":"Carga de Clientes<pre><code>print('01 (INICIO) - CARGA DE DADOS DE CLIENTES')\n\ndiretorio = fr'C:\\Temp\\Python_YT\\Git\\Projeto_BI_Zero_TO_DW\\01_SCRIPT_SQL\\DADOS_BASE\\01_clientes.csv'\n\ndfcliente = pd.read_csv(diretorio\n                    ,sep=';')\n\nwith pyodbc.connect('DRIVER={'+driver+'};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password) as conn:\n\n    with conn.cursor() as cursor:\n\n        cursor.execute(fr\"\"\"\n                        TRUNCATE TABLE dbo.TB_ITEM_VENDA;\n                        DELETE FROM dbo.TB_VENDA;\n                        DELETE FROM dbo.TB_CLIENTE;\n                        \"\"\")\n\n        for i, coluna in dfcliente.iterrows():\n\n            cursor.execute(fr\"\"\"\n                        SET IDENTITY_INSERT dbo.TB_CLIENTE ON;  \n                        INSERT INTO dbo.TB_CLIENTE (CODIGO_CLIENTE\n                                                    ,NOME_CLIENTE\n                                                    ,DATA_NASCIMENTO\n                                                    ,IDADE_CLIENTE\n                                                    ,DESCRICAO_ENDERECO\n                                                    ,NUMERO_ENDERECO\n                                                    ,NOME_CIDADE\n                                                    ,NUMERO_TELEFONE\n                                                    ,EMAIL_CLIENTE\n                                                    ,OCUPACAO_CLIENTE) \n                                                    VALUES \n                                                    ({coluna['CODIGO_CLIENTE']} \n                                                    ,'{coluna['NOME_CLIENTE']}'\n                                                    ,'{coluna['DATA_NASCIMENTO']}'\n                                                    ,'{coluna['IDADE_CLIENTE']}'\n                                                    ,'{coluna['DESCRICAO_ENDERECO']}'\n                                                    ,'{coluna['NUMERO_ENDERECO']}'\n                                                    ,'{coluna['NOME_CIDADE']}'\n                                                    ,'{coluna['NUMERO_TELEFONE']}'\n                                                    ,'{coluna['EMAIL_CLIENTE']}'\n                                                    ,'{coluna['OCUPACAO_CLIENTE']}'\n                                                    )\n                        SET IDENTITY_INSERT dbo.TB_CLIENTE OFF;\n                        \"\"\")\n\nprint('01 (FIM) - CARGA DE DADOS DE CLIENTES')\n</code></pre>"},{"location":"13_carga_sql_server_relacional/#seq-133-carga-de-vendedores","title":"SEQ-13.3 - Carga de Vendedores","text":"Carga de Vendedores<pre><code>print('02 (INICIO) - CARGA DE DADOS DE VENDEDORES')\n\ndfvendedor = pd.read_csv(fr'C:\\Temp\\Python_YT\\Git\\Projeto_BI_Zero_TO_DW\\01_SCRIPT_SQL\\DADOS_BASE\\03_vendedores.csv'\n                        ,sep=';')\n\nwith pyodbc.connect('DRIVER={'+driver+'};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password) as conn:\n\n    with conn.cursor() as cursor:\n\n        cursor.execute(fr\"\"\"DELETE FROM dbo.TB_LOJA;\n                       DELETE FROM dbo.TB_VENDEDOR;\n                       \"\"\")\n\n        for i, coluna in dfvendedor.iterrows():\n\n            cursor.execute(fr\"\"\"\n                        SET IDENTITY_INSERT dbo.TB_VENDEDOR ON;  \n                        INSERT INTO dbo.TB_VENDEDOR \n                        (CODIGO_VENDEDOR\n                        ,NOME_VENDEDOR) \n                        VALUES (\n                           {coluna['CODIGO_VENDEDOR']} \n                        , '{coluna['NOME_VENDEDOR']}'\n                        )\n                        SET IDENTITY_INSERT dbo.TB_VENDEDOR OFF;  \n                        \"\"\")\n\nprint('02 (FIM) - CARGA DE DADOS DE VENDEDORES') \n</code></pre>"},{"location":"13_carga_sql_server_relacional/#seq-134-carga-de-lojas","title":"SEQ-13.4 - Carga de Lojas","text":"Carga de Lojas<pre><code>print('03 (INICIO) - CARGA DE DADOS DE LOJAS')\n\ndfloja = pd.read_csv(fr'C:\\Temp\\Python_YT\\Git\\Projeto_BI_Zero_TO_DW\\01_SCRIPT_SQL\\DADOS_BASE\\04_lojas.csv'\n                        ,sep=';')\n\nwith pyodbc.connect('DRIVER={'+driver+'};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password) as conn:\n    with conn.cursor() as cursor:\n\n        cursor.execute(fr\"\"\"\n                        DELETE FROM dbo.TB_LOJA;\n                        \"\"\")\n\n        for i, coluna in dfloja.iterrows():\n\n            cursor.execute(fr\"\"\"\n                        SET IDENTITY_INSERT dbo.TB_LOJA ON;\n                        INSERT INTO dbo.TB_LOJA \n                        (CODIGO_LOJA_VENDEDOR\n                        ,CODIGO_LOJA\n                        ,CODIGO_VENDEDOR\n                        ,NOME_LOJA\n                        ,LOCALIDADE_LOJA\n                        ,TIPO_LOJA) \n                        VALUES (\n                        {coluna['CODIGO_LOJA_VENDEDOR']}\n                        , {coluna['CODIGO_LOJA']}\n                        , {coluna['CODIGO_VENDEDOR']}\n                        , '{coluna['NOME_LOJA']}' \n                        , '{coluna['LOCALIDADE_LOJA']}'\n                        , '{coluna['TIPO_LOJA']}')\n                        SET IDENTITY_INSERT dbo.TB_LOJA OFF;\n                        \"\"\")\n\nprint('03 (FIM) - CARGA DE DADOS DE LOJAS') \n</code></pre>"},{"location":"13_carga_sql_server_relacional/#seq-135-carga-de-categoria-de-produtos","title":"SEQ-13.5 - Carga de Categoria de Produtos","text":"Carga de Categoria de Produtos<pre><code>print('04 (INICIO) - CARGA DE DADOS DE CATEGORIA PRODUTOS')\n\ndfCategoriaProduto = pd.read_csv(fr'C:\\Temp\\Python_YT\\Git\\Projeto_BI_Zero_TO_DW\\01_SCRIPT_SQL\\DADOS_BASE\\08_categoria_produtos.csv'\n                        ,sep=';')\n\nwith pyodbc.connect('DRIVER={'+driver+'};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password) as conn:\n\n    with conn.cursor() as cursor:\n\n        cursor.execute(fr\"\"\"\n                        DELETE FROM dbo.TB_PRODUTO;\n                        DELETE FROM dbo.TB_CATEGORIA_PRODUTO;\n                        \"\"\")\n\n        for i, coluna in dfCategoriaProduto.iterrows():\n\n            cursor.execute(fr\"\"\"\n                        SET IDENTITY_INSERT dbo.TB_CATEGORIA_PRODUTO ON;  \n                        INSERT INTO dbo.TB_CATEGORIA_PRODUTO \n                           (CODIGO_CATEGORIA\n                           ,CATEGORIA_PRODUTO\n                           ,SUB_CATEGORIA_PRODUTO) \n                        VALUES (\n                           {coluna['CODIGO_CATEGORIA']} \n                        , '{coluna['CATEGORIA_PRODUTO']}'\n                        , '{coluna['SUB_CATEGORIA_PRODUTO']}'\n                        )\n                        SET IDENTITY_INSERT dbo.TB_CATEGORIA_PRODUTO OFF;  \n                        \"\"\")\n\nprint('04 (FIM) - CARGA DE DADOS DE CATEGORIA PRODUTOS')\n</code></pre>"},{"location":"13_carga_sql_server_relacional/#seq-136-carga-de-produtos","title":"SEQ-13.6 - Carga de Produtos","text":"<p>Selecionado quais bibliotecas ser\u00e3o usadas.</p> Carga de Produtos<pre><code>print('05 (INICIO) - CARGA DE DADOS DE PRODUTOS')\n\ndfproduto = pd.read_csv(fr'C:\\Temp\\Python_YT\\Git\\Projeto_BI_Zero_TO_DW\\01_SCRIPT_SQL\\DADOS_BASE\\02_produtos.csv'\n                        ,sep=';')\n\nwith pyodbc.connect('DRIVER={'+driver+'};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password) as conn:\n\n    with conn.cursor() as cursor:\n\n        cursor.execute(fr\"\"\"DELETE FROM dbo.TB_PRODUTO;\"\"\")\n\n        for i, coluna in dfproduto.iterrows():\n\n            cursor.execute(fr\"\"\"\n                        SET IDENTITY_INSERT dbo.TB_PRODUTO ON;  \n                        INSERT INTO dbo.TB_PRODUTO \n                           (CODIGO_PRODUTO\n                           , DESCRICAO_PRODUTO\n                           , CODIGO_CATEGORIA) \n                        VALUES (\n                           {coluna['CODIGO_PRODUTO']} \n                        , '{coluna['DESCRICAO_PRODUTO']}'\n                        , '{coluna['CODIGO_CATEGORIA']}'\n                        )\n                        SET IDENTITY_INSERT dbo.TB_PRODUTO OFF;  \n                        \"\"\")\n\nprint('05 (FIM) - CARGA DE DADOS DE PRODUTOS')   \n</code></pre>"},{"location":"13_carga_sql_server_relacional/#seq-137-carga-de-formas-de-pagamento","title":"SEQ-13.7 - Carga de Formas de Pagamento","text":"<p>Selecionado quais bibliotecas ser\u00e3o usadas.</p> Carga de Formas Pagamento<pre><code>print('06 (INICIO) - CARGA DE DADOS DE FORMA PAGAMENTO')\n\ndfforma_pagamento = pd.read_csv(fr'C:\\Temp\\Python_YT\\Git\\Projeto_BI_Zero_TO_DW\\01_SCRIPT_SQL\\DADOS_BASE\\05_forma_pagamento.csv'\n                        ,sep=';')\n\n#with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password) as conn:\n\nwith pyodbc.connect('DRIVER={'+driver+'};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password) as conn:\n\n    with conn.cursor() as cursor:\n\n        cursor.execute(fr\"\"\"DELETE FROM dbo.TB_FORMA_PAGAMENTO;\"\"\")        \n\n        for i, coluna in dfforma_pagamento.iterrows():\n\n            cursor.execute(fr\"\"\"\n                        SET IDENTITY_INSERT dbo.TB_FORMA_PAGAMENTO ON;  \n                        INSERT INTO dbo.TB_FORMA_PAGAMENTO \n                        (CODIGO_FORMA_PAGAMENTO\n                        ,DESCRICAO_FORMA_PAGAMENTO) \n                        VALUES (\n                        {coluna['CODIGO_FORMA_PAGAMENTO']}\n                        ,'{coluna['DESCRICAO_FORMA_PAGAMENTO']}'\n                        )\n                        SET IDENTITY_INSERT dbo.TB_FORMA_PAGAMENTO OFF;\n                        \"\"\")\n\nprint('06 (FIM) - CARGA DE DADOS DE FORMA PAGAMENTP')     \n</code></pre>"},{"location":"13_carga_sql_server_relacional/#seq-138-carga-de-vendas","title":"SEQ-13.8 - Carga de Vendas","text":"<p>Selecionado quais bibliotecas ser\u00e3o usadas.</p> Carga de Vendas<pre><code>print('07 (INICIO) - CARGA DE DADOS DE VENDAS')\n\ndfVenda = pd.read_csv(fr'C:\\Temp\\Python_YT\\Git\\Projeto_BI_Zero_TO_DW\\01_SCRIPT_SQL\\DADOS_BASE\\06_vendas.csv'\n                      ,dtype={'CODIGO_VENDA': int\n                            ,'DATA_VENDA': str\n                            ,'CODIGO_CLIENTE': int\n                            ,'CODIGO_LOJA_VENDEDOR': int\n                            ,'VALOR_FINAL': float\n                            ,'FORMA_PAGAMENTO': int\n                            ,'TIPO_PAGAMENTO': str}\n                        ,parse_dates=['DATA_VENDA']\n                        ,sep=';')\n\n\ndfVenda.sort_values(by='FORMA_PAGAMENTO')\n\nwith pyodbc.connect('DRIVER={'+driver+'};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password) as conn:\n\n    with conn.cursor() as cursor:\n\n        cursor.execute(fr\"\"\"\n                       DELETE FROM dbo.TB_ITEM_VENDA;\n                       DELETE FROM dbo.TB_VENDA;\"\"\")        \n\n        for i, coluna in dfVenda.iterrows():\n\n            cursor.execute(fr\"\"\"\n                        SET IDENTITY_INSERT dbo.TB_VENDA ON;  \n                        INSERT INTO dbo.TB_VENDA \n                        (CODIGO_VENDA\n                        ,DATA_VENDA\n                        ,CODIGO_CLIENTE\n                        ,CODIGO_LOJA_VENDEDOR\n                        ,VALOR_FINAL\n                        ,CODIGO_FORMA_PAGAMENTO\n                        ,TIPO_PAGAMENTO) \n                        VALUES (\n                        {coluna['CODIGO_VENDA']}\n                        ,'{coluna['DATA_VENDA']}'\n                        ,{coluna['CODIGO_CLIENTE']}\n                        ,{coluna['CODIGO_LOJA_VENDEDOR']}\n                        ,'{coluna['VALOR_FINAL']}'\n                        ,{coluna['FORMA_PAGAMENTO']}\n                        ,'{coluna['TIPO_PAGAMENTO']}'\n                        )\n                        SET IDENTITY_INSERT dbo.TB_VENDA OFF;\n                        \"\"\")            \n\nprint('07 (FIM) - CARGA DE DADOS DE VENDAS')\n</code></pre>"},{"location":"13_carga_sql_server_relacional/#seq-139-carga-de-itens-de-venda","title":"SEQ-13.9 - Carga de Itens de Venda","text":"<p>Selecionado quais bibliotecas ser\u00e3o usadas.</p> Carga de Itens de Venda<pre><code>print('08 (INICIO) - CARGA DE DADOS DE ITEM VENDA')\n\ndfItemVenda = pd.read_csv(fr'C:\\Temp\\Python_YT\\Git\\Projeto_BI_Zero_TO_DW\\01_SCRIPT_SQL\\DADOS_BASE\\07_item_vendas.csv'\n                      ,dtype={'CODIGO_VENDA': int\n                            ,'CODIGO_PRODUTO': str\n                            ,'VALOR_UNITARIO': float\n                            ,'QUANTIDADE': int\n                            ,'VALOR_FINAL': float}\n                        ,sep=';')\n\nwith pyodbc.connect('DRIVER={'+driver+'};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password) as conn:\n\n    with conn.cursor() as cursor:\n\n        cursor.execute(fr\"\"\"TRUNCATE TABLE dbo.TB_ITEM_VENDA;\"\"\")        \n\n        for i, coluna in dfItemVenda.iterrows():\n\n            cursor.execute(fr\"\"\"\n                        INSERT INTO dbo.TB_ITEM_VENDA \n                        (CODIGO_VENDA\n                        ,CODIGO_PRODUTO\n                        ,VALOR_UNITARIO\n                        ,QUANTIDADE\n                        ,VALOR_FINAL) \n                        VALUES (\n                        {coluna['CODIGO_VENDA']}\n                        ,{coluna['CODIGO_PRODUTO']}\n                        ,{coluna['VALOR_UNITARIO']}\n                        ,{coluna['QUANTIDADE']}\n                        ,{coluna['VALOR_FINAL']}\n                        )\n                        \"\"\")\n\nprint('08 (FIM) - CARGA DE DADOS DE ITEM VENDA')\n</code></pre>"},{"location":"13_carga_sql_server_relacional/#seq-1310-carga-de-tempo","title":"SEQ-13.10 - Carga de Tempo","text":"<p>Selecionado quais bibliotecas ser\u00e3o usadas.</p> Carga de Tempo<pre><code>print('09 (INICIO) - CARGA DE DADOS DE TEMPO')\n\ndfTempo = pd.read_csv(fr'C:\\Temp\\Python_YT\\Git\\Projeto_BI_Zero_TO_DW\\01_SCRIPT_SQL\\DADOS_BASE\\09_tempo.csv'\n                      ,dtype={'ID_TEMPO': str\n                            ,'DATA': str\n                            ,'ANO': int\n                            ,'MES': int\n                            ,'DIA': int}\n                      ,date_parser=['DATA']\n                        ,sep=';')\n\nwith pyodbc.connect('DRIVER={'+driver+'};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password) as conn:\n\n    with conn.cursor() as cursor:\n\n        cursor.execute(fr\"\"\"TRUNCATE TABLE dbo.Tempo;\"\"\")        \n\n        for i, coluna in dfTempo.iterrows():\n\n            cursor.execute(fr\"\"\"\n                        INSERT INTO dbo.Tempo \n                        (ID_TEMPO\n                        ,DATA\n                        ,ANO\n                        ,MES\n                        ,DIA) \n                        VALUES (\n                        '{coluna['ID_TEMPO']}'\n                        ,{coluna['DATA']}\n                        ,{coluna['ANO']}\n                        ,{coluna['MES']}\n                        ,{coluna['DIA']}\n                        )\n                        \"\"\")\n\nprint('09 (FIM) - CARGA DE DADOS DE TEMPO')\n</code></pre>"},{"location":"14_carga_sql_server_multidimensional/","title":"Workshop BI com Azure e seus componentes","text":""},{"location":"14_carga_sql_server_multidimensional/#14-notebook-carga-de-dados-no-axure-sql-server","title":"14 - Notebook Carga de Dados no Axure SQL Server","text":""},{"location":"14_carga_sql_server_multidimensional/#seq-141-definindo-as-bibliotecas","title":"SEQ-14.1 - Definindo as Bibliotecas","text":"<p>Selecionado quais bibliotecas ser\u00e3o usadas.</p> Bibliotecas Usadas<pre><code>from pyspark.sql.functions import *\nfrom datetime import date, timedelta\nimport pyodbc\n</code></pre>"},{"location":"14_carga_sql_server_multidimensional/#seq-142-instalacao-biblioteca-pyodbc","title":"SEQ-14.2 - Instala\u00e7\u00e3o Biblioteca pyodbc","text":"<p>Selecionado quais bibliotecas ser\u00e3o usadas.</p> Instala\u00e7\u00e3o Biblioteca pyodbc<pre><code>pip install pyodbc\n</code></pre>"},{"location":"14_carga_sql_server_multidimensional/#seq-143-importacao-arquivo-de-configuracao-no-github","title":"SEQ-14.3 - Importa\u00e7\u00e3o Arquivo de Configura\u00e7\u00e3o no GitHub","text":"<p>Processo de busca das credenciais de acesso ao Azure SQL Database armazenadas no GitHub.</p> Importa\u00e7\u00e3o Arquivo de Configura\u00e7\u00e3o no GitHub<pre><code>dfjson =  pd.read_json(\"https://raw.githubusercontent.com/dbaassists/Projeto_BI_Zero_TO_DW/main/04_ARQUIVO_CONFIG/config_azure_sql.json\")\n\nserver = dfjson['Config']['server']\ndatabase = dfjson['Config']['database']\nusername = dfjson['Config']['username']\npassword = dfjson['Config']['password']\n</code></pre>"},{"location":"14_carga_sql_server_multidimensional/#seq-144-instalacao-do-driver-odbc-driver-on-azure-databricks-cluster","title":"SEQ-14.4 - Instala\u00e7\u00e3o do Driver ODBC Driver on Azure Databricks cluster","text":"<p>Driver a ser instalado.</p> Instala\u00e7\u00e3o do Driver ODBC Driver on Azure Databricks cluster<pre><code>%sh\ncurl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -\ncurl https://packages.microsoft.com/config/ubuntu/16.04/prod.list &gt; /etc/apt/sources.list.d/mssql-release.list\nsudo apt-get update\nsudo ACCEPT_EULA=Y apt-get -q -y install msodbcsql17\n</code></pre>"},{"location":"14_carga_sql_server_multidimensional/#seq-145-limpeza-das-tabelas-do-azure-sql-database","title":"SEQ-14.5 - Limpeza das Tabelas do Azure SQL Database","text":"Limpeza das Tabelas do Azure SQL Database<pre><code>cnxn = pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER=srv-db-treinamentosql.database.windows.net;port=1433;DATABASE=srv-db-treinamentosql;UID='+username+';PWD='+ password)\n\nlista_tabela = ['dw.Fato_Venda', 'dw.Dim_Cliente', 'dw.Dim_Produto', 'dw.Dim_Forma_Pagamento', 'dw.Dim_Tempo', 'dw.Dim_Loja']\n\nfor tabela in lista_tabela:\n\n    print(tabela)\n\n    cursor = cnxn.cursor()\n\n    if tabela == 'dw.Fato_Venda':\n        comando = 'TRUNCATE TABLE'\n        print(comando)\n        cursor.execute(f'TRUNCATE TABLE {tabela}')\n\n    else:\n\n        comando = 'DELETE FROM'\n        print(comando)\n        cursor.execute(f'DELETE FROM {tabela}')    \n\n    cursor.commit()\n\ncnxn.close()    \n</code></pre>"},{"location":"14_carga_sql_server_multidimensional/#seq-146-carga-de-clientes-dwdim_cliente","title":"SEQ-14.6 - Carga de Clientes (dw.Dim_Cliente)","text":"Carga de Clientes (dw.Dim_Cliente)<pre><code># Consulta SQL para selecionar os dados do Databricks SQL\nquery = \"\"\"\nSELECT * FROM gold.dim_cliente\n\"\"\"\n# Carregando os dados do Databricks SQL\ndf = spark.sql(query)\n\n# Escrevendo os dados no banco de dados Azure SQL\ndf.write \\\n    .format(\"jdbc\") \\\n    .option(\"url\",  f\"jdbc:sqlserver://{server};database={database}\") \\\n    .option(\"dbtable\", \"dw.Dim_Cliente\") \\\n    .option(\"user\", username) \\\n    .option(\"password\", password) \\\n    .mode(\"append\") \\\n    .save()\n</code></pre>"},{"location":"14_carga_sql_server_multidimensional/#seq-147-carga-de-produtos-dwdim_produto","title":"SEQ-14.7 - Carga de Produtos (dw.Dim_Produto)","text":"Carga de Produtos (dw.Dim_Produto)<pre><code># Consulta SQL para selecionar os dados do Databricks SQL\nquery = \"\"\"\nSELECT * FROM gold.dim_produto\n\"\"\"\n# Carregando os dados do Databricks SQL\ndf = spark.sql(query)\n\n# Escrevendo os dados no banco de dados Azure SQL\ndf.write \\\n    .format(\"jdbc\") \\\n    .option(\"url\",  f\"jdbc:sqlserver://{server};database={database}\") \\\n    .option(\"dbtable\", \"dw.Dim_Produto\") \\\n    .option(\"user\", username) \\\n    .option(\"password\", password) \\\n    .mode(\"append\") \\\n    .save()\n</code></pre>"},{"location":"14_carga_sql_server_multidimensional/#seq-148-carga-de-categoria-de-forma-de-pagamento-golddim_forma_pagamento","title":"SEQ-14.8 - Carga de Categoria de Forma de Pagamento (gold.dim_forma_pagamento)","text":"Carga de Categoria de Forma de Pagamento (gold.dim_forma_pagamento)<pre><code># Consulta SQL para selecionar os dados do Databricks SQL\nquery = \"\"\"\nSELECT * FROM gold.dim_forma_pagamento\n\"\"\"\n# Carregando os dados do Databricks SQL\ndf = spark.sql(query)\n\n# Escrevendo os dados no banco de dados Azure SQL\ndf.write \\\n    .format(\"jdbc\") \\\n    .option(\"url\",  f\"jdbc:sqlserver://{server};database={database}\") \\\n    .option(\"dbtable\", \"dw.Dim_Forma_Pagamento\") \\\n    .option(\"user\", username) \\\n    .option(\"password\", password) \\\n    .mode(\"append\") \\\n    .save()\n</code></pre>"},{"location":"14_carga_sql_server_multidimensional/#seq-149-carga-de-tempo-golddim_tempo","title":"SEQ-14.9 - Carga de Tempo (gold.dim_tempo)","text":"<p>Selecionado quais bibliotecas ser\u00e3o usadas.</p> Carga de Tempo (gold.dim_tempo)<pre><code># Consulta SQL para selecionar os dados do Databricks SQL\nquery = \"\"\"\nSELECT * FROM gold.dim_tempo\n\"\"\"\n# Carregando os dados do Databricks SQL\ndf = spark.sql(query)\n\n# Escrevendo os dados no banco de dados Azure SQL\ndf.write \\\n    .format(\"jdbc\") \\\n    .option(\"url\",  f\"jdbc:sqlserver://{server};database={database}\") \\\n    .option(\"dbtable\", \"dw.Dim_Tempo\") \\\n    .option(\"user\", username) \\\n    .option(\"password\", password) \\\n    .mode(\"append\") \\\n    .save()\n</code></pre>"},{"location":"14_carga_sql_server_multidimensional/#seq-1410-carga-de-formas-de-loja-dwdim_loja","title":"SEQ-14.10 - Carga de Formas de Loja (dw.Dim_loja)","text":"<p>Selecionado quais bibliotecas ser\u00e3o usadas.</p> Carga de Formas de Loja (dw.Dim_loja)<pre><code># Consulta SQL para selecionar os dados do Databricks SQL\nquery = \"\"\"\nSELECT * FROM gold.dim_loja\n\"\"\"\n# Carregando os dados do Databricks SQL\ndf = spark.sql(query)\n\n# Escrevendo os dados no banco de dados Azure SQL\ndf.write \\\n    .format(\"jdbc\") \\\n    .option(\"url\",  f\"jdbc:sqlserver://{server};database={database}\") \\\n    .option(\"dbtable\", \"dw.Dim_Loja\") \\\n    .option(\"user\", username) \\\n    .option(\"password\", password) \\\n    .mode(\"append\") \\\n    .save()   \n</code></pre>"},{"location":"14_carga_sql_server_multidimensional/#seq-148-carga-de-vendas-dwfato_venda","title":"SEQ-14.8 - Carga de Vendas (dw.Fato_Venda)","text":"<p>Selecionado quais bibliotecas ser\u00e3o usadas.</p> Carga de Vendas (dw.Fato_Venda)<pre><code># Consulta SQL para selecionar os dados do Databricks SQL\nquery = \"\"\"\nSELECT * FROM gold.fato_venda\n\"\"\"\n# Carregando os dados do Databricks SQL\ndf = spark.sql(query)\n\n# Escrevendo os dados no banco de dados Azure SQL\ndf.write \\\n    .format(\"jdbc\") \\\n    .option(\"url\",  f\"jdbc:sqlserver://{server};database={database}\") \\\n    .option(\"dbtable\", \"dw.Fato_Venda\") \\\n    .option(\"user\", username) \\\n    .option(\"password\", password) \\\n    .mode(\"append\") \\\n    .save()\n</code></pre>"},{"location":"15_arquivos/","title":"Workshop BI com Azure e seus componentes","text":""},{"location":"15_arquivos/#14-arquivos-do-workshop","title":"14 - Arquivos do Workshop","text":""},{"location":"15_arquivos/#seq-14-sessao-de-arquivos","title":"SEQ-14 - Sess\u00e3o de Arquivos","text":"<p>Durante o treinamento, todos os arqivos gerados ser\u00e3o disponibilizados para voc\u00ea! </p> <p>Essa sess\u00e3o fica respons\u00e1vel por assumir a fun\u00e7\u00e3o de cat\u00e1logo. Estaremos construindo ela a medida que o workshop for avan\u00e7ando! </p>"},{"location":"15_arquivos_original/","title":"Workshop BI com Azure e seus componentes","text":""},{"location":"15_arquivos_original/#15-arquivos-do-workshop","title":"15 - Arquivos do Workshop","text":""},{"location":"15_arquivos_original/#seq-15-sessao-de-arquivos","title":"SEQ-15 - Sess\u00e3o de Arquivos","text":"<p>Durante o treinamento, todos os arqivos gerados ser\u00e3o disponibilizados para voc\u00ea! </p> <p>Essa sess\u00e3o fica respons\u00e1vel por assumir a fun\u00e7\u00e3o de cat\u00e1logo. Estaremos construindo ela a medida que o workshop for avan\u00e7ando! </p>"},{"location":"15_arquivos_original/#para-visualizar-o-arquivo-da-arquitetura-do-projeto-seq-03-e-ncessario-acessar-o-excalidraw","title":"Para visualizar o arquivo da arquitetura do projeto (SEQ-03), \u00e9 ncess\u00e1rio acessar o Excalidraw","text":"Sequ\u00eancia Arquivo Link SEQ-01 Arquivo Cria\u00e7\u00e3o Tabelas SQL Arquivo CREATE TABLE SEQ-02 Arquivo Inser\u00e7\u00e3o Dados nas Tabelas SQL Arquivo INSERT DATA SEQ-03 Arquivo da Arquitetura do Projeto Arquivo Arquitetura SEQ-04 Arquivo Excel de Exemplo do Cen\u00e1rio Arquivo Base Excel SEQ-05 Arquivo Configura\u00e7\u00e3o Ambiente Databricks Notebook Configura\u00e7\u00e3o SEQ-06 Arquivo Exporta\u00e7\u00e3o Dados Azure SQL Database Notebook Extra\u00e7\u00e3o Dados SEQ-07 Arquivo JSON de Parametriza\u00e7\u00e3o de Conex\u00e3o com Azure SQL Database JSON de Configura\u00e7\u00e3o SEQ-08 Arquivo Ingest\u00e3o Dados Camada Bronze Notebook Carga Camada Bronze SEQ-09 Arquivo Ingest\u00e3o Dados Camada Silver Notebook Carga Camada Silver SEQ-10 Arquivo Ingest\u00e3o Dados Camada Gold Notebook Carga Camada Gold SEQ-11 Arquivo Carga Dados Azure SQL Database Schema DW Notebook Carga DW Azure SQL Database SEQ-12 Arquivo An\u00e1lise de Dados Databricks Notebook An\u00e1lise dos Dados SEQ-13 Arquivo Python Gera\u00e7\u00e3o Massa Dados Gera\u00e7\u00e3o Massa Dados SEQ-14 Arquivo Carga de Dados no SQL Server Carga de Dados no SQL Server SEQ-15 Arquivo Modelo Dados Relacional Toad Data Modeler Modelo Dados Banco Relacional Toad Data Modeler SEQ-16 Arquivo Modelo Dados Dimensional Toad Data Modeler Modelo Dados Banco Dimensional Toad Data Modeler SEQ-17 Arquivo Cria\u00e7\u00e3o Tabelas Fato e Dimens\u00f5es SQL Arquivo CREATE TABLE Fato e Dimens\u00f5es"},{"location":"16_formulario/","title":"Workshop BI com Azure e seus componentes","text":""},{"location":"16_formulario/#15-formulario-de-inscricao","title":"15 - Formul\u00e1rio de Inscri\u00e7\u00e3o","text":"<p>Bem, amigo, caso tenha gostado da proposta, basta preencher o formul\u00e1rio abaixo para confirmar sua inscri\u00e7\u00e3o.</p> <p>O investimento ser\u00e1 de R$ 250,00, podendo ser pago via PIX ou cart\u00e3o de cr\u00e9dito.</p> <p>No workshop, abordaremos os seguintes assuntos:</p> <p>1 - Falar sobre a import\u00e2ncia dos dados nos dias de hoje;  2 - Conhecer o ecossistema de cloud da Microsoft, o Azure;  3 - Abordar conceitos de modelagem dimensional e arquitetura de dados;  4 - Conhecer e explorar o que \u00e9 ETL e ELT;  5 - Falar sobre Data Warehouse, Data Lake, Delta Table e Data Lakehouse;  6 - Criar uma conta no Azure;  7 - Provisionar recursos (Resource Group, Azure Storage Account, Azure Data Factory, Azure Key Vault, Azure SQL Database, Azure Databricks);  8 - Conhecer e configurar os tipos de Integration Runtime (IR) e saber como e quando devemos us\u00e1-los;  9 - Aprender a trabalhar com recursos externos do Azure, Azure Storage Explorer e Azure Data Studio;  10 - Trabalhar com CI/CD, com Azure DevOps e GitHub. Entender como essas solu\u00e7\u00f5es podem auxiliar em um ambiente corporativo;  11 - Explorar o Azure Data Factory, criar Linked Services, DataSets, Pipelines;  12 - Desbravar o Azure Databricks;  13 - Aplicar Data Quality em um projeto de dados, podendo assim conhecer de perto seus benef\u00edcios e como ele pode ajudar no dia a dia das empresas;  14 - Trabalhar com o Azure Key Vault para garantir conceitos de seguran\u00e7a;  15 - Explorar um projeto de ponta a ponta.  16 - Al\u00e9m de tudo isso, estarei trazendo experi\u00eancias de projeto anteriores, abordando principalmente falhas que cometi para que voc\u00ea n\u00e3o fa\u00e7a o mesmo. </p> <p>Para visualizar o o formul\u00e1rio, \u00e9 ncess\u00e1rio acessar o link Formul\u00e1rio de Inscri\u00e7\u00e3o</p> <p>Proveito para deixar meus contatos.</p> <p>E-mail: dbaassists@gmail.com  Intagram - https://www.instagram.com/dbaassists/  Twitter - https://twitter.com/dbaassists  YouTube - https://www.youtube.com/@quintellao/featured  Blog DbaAssists - https://www.dbaassists.com.br/  GitHub - https://github.com/dbaassists </p>"},{"location":"1_projeto/","title":"Workshop BI com Azure e seus componentes","text":""},{"location":"1_projeto/#1-como-faremos-isso-acontecer","title":"1 - Como faremos isso acontecer?","text":"<p>Com base em projetos trabalhados, irei focar nessa stack do Azure.</p> <p>Sob um olha macro:</p> <p></p> <p>Mergulhando em seu funcionamento, o fluxo ser\u00e1 o seguinte:</p> <p></p> <p>Para alcan\u00e7ar o objetivo final do workshop, abordaremos os seguintes assuntos:</p> <p>1 - Falar sobre a import\u00e2ncia dos dados nos dias de hoje;  2 - Conhecer o ecossistema de cloud da Microsoft, o Azure;  3 - Abordar conceitos de modelagem dimensional e arquitetura de dados;  4 - Conhecer e explorar o que \u00e9 ETL e ELT;  5 - Falar sobre Data Warehouse, Data Lake, Delta Table e Data Lakehouse;  6 - Criar uma conta no Azure;  7 - Provisionar recursos (Resource Group, Azure Storage Account, Azure Data Factory, Azure Key Vault, Azure SQL Database, Azure Databricks);  8 - Conhecer e configurar os tipos de Integration Runtime (IR) e saber como e quando devemos us\u00e1-los;  9 - Aprender a trabalhar com recursos externos do Azure, Azure Storage Explorer e Azure Data Studio;  10 - Trabalhar com CI/CD, com Azure DevOps e GitHub. Entender como essas solu\u00e7\u00f5es podem auxiliar em um ambiente corporativo;  11 - Explorar o Azure Data Factory, criar Linked Services, DataSets, Pipelines;  12 - Desbravar o Azure Databricks;  13 - Aplicar Data Quality em um projeto de dados, podendo assim conhecer de perto seus benef\u00edcios e como ele pode ajudar no dia a dia das empresas;  14 - Trabalhar com o Azure Key Vault para garantir conceitos de seguran\u00e7a;  15 - Explorar um projeto de ponta a ponta.  16 - Al\u00e9m de tudo isso, estarei trazendo experi\u00eancias de projeto anteriores, abordando principalmente falhas que cometi para que voc\u00ea n\u00e3o fa\u00e7a o mesmo. </p> <p>Ap\u00f3s abordar a parte conceitual do nosso workshow, vamos realizar uma explora\u00e7\u00e3o do ambiente de cloud da Microsoft, o famoso Azure!</p> <p>E para isso ocorrer vamos precisar seguir os seguintes passos:</p> Sequ\u00eancia Objeto Detalhamento SEQ-01 Cria\u00e7\u00e3o de Conta no Azure SEQ-02 Conhecer o Ecosistema SEQ-03 Provisionar os recursos <p>O passo seguinte ser\u00e1 realizar o aprofundamento nesses recursos criados para poder entender por que estamos usando, abordar o funcionamento e entender o beneficio de cada um deles dentro do projeto.</p> Sequ\u00eancia Objeto Detalhamento SEQ-01 Subscription SEQ-02 Azure Resource Group SEQ-03 Azure Storage Account SEQ-04 Azure SQL Database SEQ-05 Azure Data Factory SEQ-06 Azure Key Vault <p>Quando o assunto for Azure Data Factory, vamos focar em alguns pontos:</p> Sequ\u00eancia Objeto Detalhamento SEQ-01 Linked Service SEQ-02 DataSet SEQ-03 Pipeline SEQ-04 Monitoramento SEQ-05 Git - GitHub e Azure DevOps SEQ-06 Integration Runtime SEQ-07 Azure Key Vault <p>E quando o assunto for Databricks...</p> Sequ\u00eancia Objeto Detalhamento SEQ-01 Cria\u00e7\u00e3o de Cluster SEQ-02 Arquitetura Medalh\u00e3o SEQ-03 Notebook SEQ-04 Git - GitHub e Azure DevOps SEQ-05 Criando uma Repos, uma Branch e uma Pull Request SEQ-06 Jobs SEQ-07 Execu\u00e7\u00e3o de Notebooks Via Azure Data Factory SEQ-08 Data Quality SEQ-09 Monitoramento"},{"location":"2_provisionando_recursos_azure/","title":"Workshop BI com Azure e seus componentes","text":"<p>Essa etapa \u00e9 respons\u00e1vel por explicar os recursos que ser\u00e3o usados durante o treinamento.</p> <p></p>"},{"location":"2_provisionando_recursos_azure/#21-recursos-que-serao-usados-da-cloud-azure","title":"2.1 - Recursos que ser\u00e3o usados da Cloud Azure","text":"<p>Pr\u00e9-Requisito para o andamnento do projeto.</p> Recurso Recurso Azure Nome Recurso Projeto Descri\u00e7\u00e3o Subscription ss-workshop-azure Uma \"subscription\" (assinatura) da Microsoft Azure \u00e9 um contrato com a Microsoft que permite aos usu\u00e1rios acessar e usar os servi\u00e7os e recursos da plataforma Azure. Azure Resource Group rg-workshop-azure Um Azure Resource Group (Grupo de Recursos do Azure) \u00e9 um cont\u00eainer l\u00f3gico que agrupa recursos relacionados em uma assinatura da Microsoft Azure. Ele \u00e9 usado para gerenciar e organizar recursos de maneira l\u00f3gica e coesa. Azure Storage Account asaworkshopazure Uma conta de armazenamento do Azure (Azure Storage Account) \u00e9 um servi\u00e7o central da plataforma de nuvem da Microsoft, Azure, que fornece armazenamento altamente dispon\u00edvel, seguro, dur\u00e1vel, escal\u00e1vel e acess\u00edvel na nuvem. Azure Data Factory adf-workshop-azure O Azure Data Factory \u00e9 um servi\u00e7o de integra\u00e7\u00e3o de dados totalmente gerenciado pela Microsoft Azure que permite criar, agendar e orquestrar fluxos de trabalho de movimenta\u00e7\u00e3o e transforma\u00e7\u00e3o de dados em larga escala. Azure SQL Database asqldb-workshop-azure O Azure SQL Database \u00e9 um servi\u00e7o de banco de dados relacional totalmente gerenciado oferecido pela Microsoft Azure. Ele \u00e9 baseado na popular plataforma de banco de dados SQL Server da Microsoft e fornece uma solu\u00e7\u00e3o de banco de dados na nuvem altamente escal\u00e1vel, segura e de alto desempenho. Azure Key Vault akv-workshop-azure O Azure Key Vault \u00e9 um servi\u00e7o de gerenciamento de segredos e chaves na Microsoft Azure, projetado para proteger, armazenar e controlar o acesso a chaves de criptografia, senhas, certificados e outros segredos confidenciais usados por aplicativos e servi\u00e7os na nuvem. Azure Databricks adb-workshop-azure O Azure Databricks \u00e9 um servi\u00e7o de an\u00e1lise unificada baseado na nuvem que combina o poder do Apache Spark com uma plataforma de colabora\u00e7\u00e3o e integra\u00e7\u00e3o. Desenvolvido em parceria com a Databricks e a Microsoft, o Azure Databricks permite que equipes de an\u00e1lise e engenharia de dados colaborem em projetos de big data e an\u00e1lise de dados de forma eficiente e escal\u00e1vel."},{"location":"2_provisionando_recursos_azure/#22-recursos-extras","title":"2.2 - Recursos Extras","text":"Recurso Recurso Nome do Recurso Descri\u00e7\u00e3o Framework Data Quality Framework Uma solu\u00e7\u00e3o de data quality, ou qualidade de dados, refere-se a um conjunto de pr\u00e1ticas, processos e ferramentas utilizadas para garantir que os dados em um sistema ou organiza\u00e7\u00e3o estejam completos, precisos, consistentes, atualizados e relevantes para o prop\u00f3sito pretendido. GitHub prj_azure_git GitHub \u00e9 uma plataforma de desenvolvimento de software baseada na web que oferece controle de vers\u00e3o Git e ferramentas de colabora\u00e7\u00e3o para desenvolvedores. Azure DevOps prj_azure_devops Azure DevOps \u00e9 um conjunto de servi\u00e7os de colabora\u00e7\u00e3o baseados na nuvem que permitem planejar, desenvolver, testar e implantar aplicativos de software com efici\u00eancia. Ele fornece uma plataforma integrada para gerenciamento de projetos de software e ciclo de vida de desenvolvimento, permitindo que equipes de desenvolvimento entreguem software de alta qualidade de maneira r\u00e1pida e cont\u00ednua. Azure Data Studio Azure Data Studio O Azure Data Studio \u00e9 uma ferramenta de gerenciamento de dados multiplataforma, gratuita e de c\u00f3digo aberto, desenvolvida pela Microsoft. Ela oferece uma experi\u00eancia unificada para desenvolvedores e administradores de banco de dados trabalharem com uma variedade de sistemas de gerenciamento de banco de dados, incluindo SQL Server, Azure SQL Database, PostgreSQL e MySQL. Azure Storage Explorer Azure Storage Explorer O Azure Storage Explorer \u00e9 uma ferramenta gratuita e independente desenvolvida pela Microsoft que permite aos usu\u00e1rios gerenciar recursos de armazenamento do Azure de forma eficiente e intuitiva. Ele oferece uma interface gr\u00e1fica f\u00e1cil de usar para acessar e interagir com contas de armazenamento, blobs, tabelas, filas e arquivos no Azure."},{"location":"3_modelagem_dados/","title":"Workshop BI com Azure e seus componentes","text":""},{"location":"3_modelagem_dados/#3-falando-um-pouco-sobre-modelagem-dimensional-ou-multidimensional","title":"3 - Falando um pouco sobre Modelagem Dimensional ou Multidimensional","text":"<p>Nessa etapa do treinamneto, iremos abordar conceitos valios\u00edssimos no que diz respeito a modelagem de dados.</p> <p>Voc\u00ea que pretende ou j\u00e1 trabalhar em projetos de BI ou Big Data, sabe muito bem que todo projeto possui uma modelagem multidimensional e que isso impacta diretamente na camada de apresenta\u00e7\u00e3o das informa\u00e7\u00f5es.</p> <p>Ent\u00e3o, pensando nisso que acabei de falar, destaco os pontos que estarei abordando nesse workshop:</p> Sequ\u00eancia D\u00favida Resposta 1 O que \u00e9 essa tal de Modelagem de Dados Multidimensional? ? 2 O que \u00e9 e para que server uma Dimens\u00e3o? ? 3 \u00c9 verdade que todo modelo multidimensional deve conter uma Dimens\u00e3o de Tempo? ? 4 O que \u00e9 uma tabela Fato? Para que serve? ? 5 Quais s\u00e3o os modelos mais usados em uma Modelagem de Dados Multidimensional? ? 6 J\u00e1 ouvi falar sobre um tal de SCD, o que \u00e9 isso? Para que serve? Quando devo usar? ?"},{"location":"3_modelagem_dados/#31-exemplo-de-modelagem-de-dados-multidimensional","title":"3.1 - Exemplo de Modelagem de Dados Multidimensional","text":""},{"location":"4_etl_elt/","title":"Workshop BI com Azure e seus componentes","text":""},{"location":"4_etl_elt/#4-falando-um-pouco-sobre-etl-e-elt","title":"4 - Falando um pouco sobre ETL e ELT","text":"<p>Outro ponto que merece sua aten\u00e7\u00e3o \u00e9 o assunto relacionado a ETL e ETL.</p> <p>Ainda existem muitas dificuldades de compreender o seu funcionamento, quando devo usar, quais s\u00e3o as principais diferen\u00e7as entre eles e at\u00e9 outras...</p> <p>No nosso Workshop, existir\u00e1 um t\u00f3pico espec\u00edfico e exclusivo para falarmos sobre esse assunto e eu garanto que voc\u00ea ir\u00e1 sair sabendo responder as perguntas abaixo.</p> Sequ\u00eancia D\u00favida Resposta 1 O que \u00e9 um ETL e um ELT? ? 2 Quando devo aplica-los? ? 3 Quais etapas comp\u00f5em cada fase do processo? ? 4 Posso ter um projeto de BI sem esse tal de ETL ou ELT? ? 5 Quais s\u00e3o as diferen\u00e7as entre ETL e ELT? ? 6 \u00c9 caro construir e manter um processo de ETL ou ELT? ? 7 Quais s\u00e3o os componentes mais comuns do fluxo de ETL e ELT? ?"},{"location":"4_etl_elt/#41-representacao-grafica-do-fluxo-de-etl","title":"4.1 - Representa\u00e7\u00e3o Gr\u00e1fica do Fluxo de ETL","text":""},{"location":"4_etl_elt/#42-representacao-grafica-do-fluxo-de-elt","title":"4.2 - Representa\u00e7\u00e3o Gr\u00e1fica do Fluxo de ELT","text":""},{"location":"5_arquitetura_dados/","title":"Workshop BI com Azure e seus componentes","text":""},{"location":"5_arquitetura_dados/#5-arquitetura","title":"5 - Arquitetura","text":"<p>E quando entrarmos no assunto arquitetura de dados, o que podemos esperar?</p> <p>Muita coisa, isso eu j\u00e1 posso adiantar pra voc\u00ea!</p> <p>Vou tentar destacar alguns pontos, ok?</p> Sequ\u00eancia D\u00favida Resposta 1 Quais s\u00e3o as suas fases? ? 2 Quando estarei pronto para construir? ? 3 O que ela representar dentro de um projeto de BI e Big Data? ? 4 Existe diferen\u00e7as entre o mundo OnPremises e Cloud? ? 5 Quais componentes fazem parte de uma Arquitetura de Dados? ? <p>Eis um pequeno aperitivo...</p>"},{"location":"5_arquitetura_dados/#51-representacao-grafica-de-uma-arquitetura-bi-ou-arquitetura-on-premise","title":"5.1 - Representa\u00e7\u00e3o Gr\u00e1fica de uma Arquitetura BI ou Arquitetura On-Premise","text":""},{"location":"5_arquitetura_dados/#52-representacao-grafica-de-uma-arquitetura-big-data-ou-modern-data-architecture","title":"5.2 - Representa\u00e7\u00e3o Gr\u00e1fica de uma Arquitetura Big Data ou Modern Data Architecture","text":""},{"location":"6_arquitetura_medalhao/","title":"Workshop BI com Azure e seus componentes","text":""},{"location":"6_arquitetura_medalhao/#6-arquitetura-medalhao","title":"6 - Arquitetura Medalh\u00e3o","text":"<p>Caso tivesse que definir a arquitetura medalh\u00e3o em poucas palavras eu poderia colocar o seguinte:</p> <p>\"A arquitetura medalh\u00e3o \u00e9 um padr\u00e3o arquitetural comumente utilizado em sistemas de processamento de dados distribu\u00eddos.</p> <p>Onde Sistemas de processamento de dados distribu\u00eddos se referem a sistemas de computa\u00e7\u00e3o que processam grandes volumes de dados divididos em m\u00faltiplos componentes ou n\u00f3s de processamento, que podem estar localizados em diferentes m\u00e1quinas f\u00edsicas ou em uma infraestrutura de nuvem distribu\u00edda.\"</p> <p>Vamos falar sobre:</p> <p></p> Sequ\u00eancia Pergunta Resposta 1 O que \u00e9 a arquitetura medalh\u00e3o? ? 2 Qual a vantagem da sua implanta\u00e7\u00e3o? ? 3 Quando/Por que preciso criar a Landing Zone? ? <p>Representa\u00e7\u00e3o Gr\u00e1fica de uma Arquitetura Medalh\u00e3o</p> <p></p> <p>Arquitetura que iremos implementar.</p> <p></p>"},{"location":"7_dw_data_lake_lakehouse/","title":"Workshop BI com Azure e seus componentes","text":""},{"location":"7_dw_data_lake_lakehouse/#7-o-que-conseguiremos-compreender-depois-desse-capitulo","title":"7 - O que conseguiremos compreender depois desse cap\u00edtulo?","text":"<p>Entramos nesse momento em uma das parte mais importantes do nosso Workshop.</p> <p>Vamos falar um pouco sobre Data Warehouse, Data Lake, Delta Tables e Data Lakehouse.</p> <p>Vamos entender de forma macro alguns conceitos:</p> <ul> <li> <p>Um Data Warehouse \u00e9 um sistema de armazenamento centralizado e otimizado para an\u00e1lise de dados hist\u00f3ricos e operacionais. </p> </li> <li> <p>Um Data Lake \u00e9 um reposit\u00f3rio de dados que armazena grandes volumes de dados brutos e n\u00e3o processados, em sua maioria em formato n\u00e3o estruturado ou semi-estruturado. </p> </li> <li> <p>Uma Delta Table \u00e9 uma tabela de dados distribu\u00edda que utiliza o formato Delta Lake, um formato de armazenamento de dados otimizado para an\u00e1lises de big data em ambientes de nuvem. Ela combina as caracter\u00edsticas de um Data Lake com os benef\u00edcios de um Data Warehouse, oferecendo a capacidade de armazenar dados brutos e processados em um \u00fanico local.</p> </li> <li> <p>Um Data Lakehouse \u00e9 uma arquitetura de dados que combina os conceitos de Data Lake e Data Warehouse em um \u00fanico ambiente. Ele integra as capacidades de armazenamento flex\u00edvel e escal\u00e1vel do Data Lake com a capacidade de consulta e an\u00e1lise estruturada do Data Warehouse. O Data Lakehouse \u00e9 projetado para oferecer flexibilidade, escalabilidade e desempenho para an\u00e1lises de big data, permitindo que as organiza\u00e7\u00f5es processem e analisem grandes volumes de dados de forma eficiente e confi\u00e1vel.</p> </li> </ul> <p></p> Sequ\u00eancia D\u00favida Resposta 1 O que \u00e9 um Data Warehouse? ? 2 O que \u00e9 um Data Mart? ? 3 O que \u00e9 um Data Lake? ? 4 Por que preciso ter um Data Lake e n\u00e3o um Data Warehouse? ? 5 Data Lake \u00e9 a mesma coisa que Data Warehouse? ? 6 Quais s\u00e3o as diferen\u00e7as entre um Data Warehouse e um Data Lake? ? 7 O que \u00e9 o Data LakeHouse? ? 8 O que \u00e9 o Delta Lake? ? <p>Vamos a alguns exemplos...</p> <p>Representa\u00e7\u00e3o Gr\u00e1fica de um Data Warehouse</p> <p></p> <p>Representa\u00e7\u00e3o Gr\u00e1fica de um Data Lake</p> <p></p> <p>Representa\u00e7\u00e3o Gr\u00e1fica de um Data Lakehouse</p> <p></p> <p>Evolu\u00e7\u00e3o do Data Warehouse para o Data Lakehouse</p> <p></p>"},{"location":"8_seguranca/","title":"Workshop BI com Azure e seus componentes","text":""},{"location":"8_seguranca/#8-seguranca-em-uma-pipeline-de-dados","title":"8 - Seguran\u00e7a em uma Pipeline de Dados","text":"<p>Como podemos garantir as melhores pr\u00e1ticas de seguran\u00e7a em nossa arquitetura de dados?</p> <p>Dentro do ecosistema da Azure, existe um recurso chamado Azure Key Vault. </p> <p>Esse recurso ele funciona como um confre de senhas e credenciais de acesso. </p> <p>Durante o nosso Workshop, estarei apresentando de forma bem detalhada como estaremos fazendo a sua utiliza\u00e7\u00e3o para gerenciar nossas credenciais e com isso garantir que o acesso aos nossos recursos sejam realizados de forma gerenciada.</p> <p></p> <p></p> <p></p> Recurso Recurso Azure Nome Recurso Projeto Descri\u00e7\u00e3o Azure Key Vault akv-workshop-azure O Azure Key Vault \u00e9 um servi\u00e7o de gerenciamento de segredos e chaves na Microsoft Azure, projetado para proteger, armazenar e controlar o acesso a chaves de criptografia, senhas, certificados e outros segredos confidenciais usados por aplicativos e servi\u00e7os na nuvem."},{"location":"9_cicd/","title":"Workshop BI com Azure e seus componentes","text":""},{"location":"9_cicd/#9-trabalhando-com-cicd","title":"9 - Trabalhando com CI/CD","text":"<p>Quando falamos de CI/CD (Continuous Integration/Continuous Delivery), podemos dizer que para dados \u00e9 uma pr\u00e1tica que visa automatizar e acelerar o processo de desenvolvimento, teste e implanta\u00e7\u00e3o de pipelines de dados e an\u00e1lises. </p> <p>Envolvendo a automa\u00e7\u00e3o de tarefas como ingest\u00e3o de dados, limpeza, transforma\u00e7\u00e3o, valida\u00e7\u00e3o e carga (ETL/ELT), bem como a execu\u00e7\u00e3o de testes automatizados e a implanta\u00e7\u00e3o de pipelines de dados em ambientes de produ\u00e7\u00e3o.</p> <p>Esses conceitos tamb\u00e9m s\u00e3o aplicados para desenvolvimentos de software.</p> <p>Em nosso Workshop estaremos usando o GitHub e o Azure DevOps para que voc\u00ea possa ver como que funciona no Azure Data Factory e no Azure Databricks.</p>"},{"location":"9_cicd/#91-github","title":"9.1 - GitHub","text":"Recurso Recurso Nome do Recurso Descri\u00e7\u00e3o GitHub prj_azure_git GitHub \u00e9 uma plataforma de desenvolvimento de software baseada na web que oferece controle de vers\u00e3o Git e ferramentas de colabora\u00e7\u00e3o para desenvolvedores."},{"location":"9_cicd/#92-azure-devops","title":"9.2 - Azure DevOps","text":"Recurso Recurso Nome do Recurso Descri\u00e7\u00e3o Azure DevOps prj_azure_devops Azure DevOps \u00e9 um conjunto de servi\u00e7os de colabora\u00e7\u00e3o baseados na nuvem que permitem planejar, desenvolver, testar e implantar aplicativos de software com efici\u00eancia. Ele fornece uma plataforma integrada para gerenciamento de projetos de software e ciclo de vida de desenvolvimento, permitindo que equipes de desenvolvimento entreguem software de alta qualidade de maneira r\u00e1pida e cont\u00ednua."}]}